{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Size: (12,)\n",
      "Drone starting at: (29.96, 35.67) m\n",
      "Environment Reset. Wind Dir: 266.6 deg. Moving Platform: True\n",
      "Drone starting at: (21.97, 42.90) m\n",
      "Environment Reset. Wind Dir: 307.7 deg. Moving Platform: True\n",
      "Step: 0, Reward: 0.15395180881023407, Done: False\n",
      "Step: 1, Reward: 0.1535642296075821, Done: False\n",
      "Step: 2, Reward: 0.15690112113952637, Done: False\n",
      "Step: 3, Reward: 0.15363837778568268, Done: False\n",
      "Step: 4, Reward: 0.15025538206100464, Done: False\n",
      "Step: 5, Reward: 0.15045183897018433, Done: False\n",
      "Step: 6, Reward: 0.144052654504776, Done: False\n",
      "Step: 7, Reward: 0.1467447727918625, Done: False\n",
      "Step: 8, Reward: 0.14786292612552643, Done: False\n",
      "Step: 9, Reward: 0.14492163062095642, Done: False\n",
      "Step: 10, Reward: 0.14201495051383972, Done: False\n",
      "Step: 11, Reward: 0.13707268238067627, Done: False\n",
      "Step: 12, Reward: 0.13520871102809906, Done: False\n",
      "Step: 13, Reward: 0.13449610769748688, Done: False\n",
      "Step: 14, Reward: 0.13300608098506927, Done: False\n",
      "Step: 15, Reward: 0.1352774053812027, Done: False\n",
      "Step: 16, Reward: 0.13225513696670532, Done: False\n",
      "Step: 17, Reward: 0.13438427448272705, Done: False\n",
      "Step: 18, Reward: 0.13243991136550903, Done: False\n",
      "Step: 19, Reward: 0.1308092325925827, Done: False\n",
      "Step: 20, Reward: 0.1349915862083435, Done: False\n",
      "Step: 21, Reward: 0.13491696119308472, Done: False\n",
      "Step: 22, Reward: 0.13301751017570496, Done: False\n",
      "Step: 23, Reward: 0.12614654004573822, Done: False\n",
      "Step: 24, Reward: 0.1223316490650177, Done: False\n",
      "Step: 25, Reward: 0.12297789752483368, Done: False\n",
      "Step: 26, Reward: 0.12431017309427261, Done: False\n",
      "Step: 27, Reward: 0.12221090495586395, Done: False\n",
      "Step: 28, Reward: 0.1272813379764557, Done: False\n",
      "Step: 29, Reward: 0.13076800107955933, Done: False\n",
      "Step: 30, Reward: 0.1350204199552536, Done: False\n",
      "Step: 31, Reward: 0.14245091378688812, Done: False\n",
      "Step: 32, Reward: 0.14462293684482574, Done: False\n",
      "Step: 33, Reward: 0.14313313364982605, Done: False\n",
      "Step: 34, Reward: 0.14578001201152802, Done: False\n",
      "Step: 35, Reward: 0.1422683596611023, Done: False\n",
      "Step: 36, Reward: 0.14294421672821045, Done: False\n",
      "Step: 37, Reward: 0.13779979944229126, Done: False\n",
      "Step: 38, Reward: 0.12941041588783264, Done: False\n",
      "Step: 39, Reward: 0.12198705971240997, Done: False\n",
      "Step: 40, Reward: 0.11386676877737045, Done: False\n",
      "Step: 41, Reward: 0.11223196983337402, Done: False\n",
      "Step: 42, Reward: 0.10844141244888306, Done: False\n",
      "Step: 43, Reward: 0.101705402135849, Done: False\n",
      "Step: 44, Reward: 0.09786276519298553, Done: False\n",
      "Step: 45, Reward: 0.09196546673774719, Done: False\n",
      "Step: 46, Reward: 0.08252261579036713, Done: False\n",
      "Step: 47, Reward: 0.07851790636777878, Done: False\n",
      "Step: 48, Reward: 0.07739050686359406, Done: False\n",
      "Step: 49, Reward: 0.07455071061849594, Done: False\n",
      "Step: 50, Reward: 0.06796335428953171, Done: False\n",
      "Step: 51, Reward: 0.06235495209693909, Done: False\n",
      "Step: 52, Reward: 0.06039426475763321, Done: False\n",
      "Step: 53, Reward: 0.059394948184490204, Done: False\n",
      "Step: 54, Reward: 0.06241708993911743, Done: False\n",
      "Step: 55, Reward: 0.05814468860626221, Done: False\n",
      "Step: 56, Reward: 0.054870545864105225, Done: False\n",
      "Step: 57, Reward: 0.05006275326013565, Done: False\n",
      "Step: 58, Reward: 0.041262075304985046, Done: False\n",
      "Step: 59, Reward: 0.04131792485713959, Done: False\n",
      "Step: 60, Reward: 0.041316017508506775, Done: False\n",
      "Step: 61, Reward: 0.03209869563579559, Done: False\n",
      "Step: 62, Reward: 0.032565467059612274, Done: False\n",
      "Step: 63, Reward: 0.02203422039747238, Done: False\n",
      "Step: 64, Reward: 0.01630491018295288, Done: False\n",
      "Step: 65, Reward: 0.01256551593542099, Done: False\n",
      "Step: 66, Reward: 0.005491890013217926, Done: False\n",
      "Step: 67, Reward: -0.0029931366443634033, Done: False\n",
      "Step: 68, Reward: -0.01527538150548935, Done: False\n",
      "Step: 69, Reward: -0.022990919649600983, Done: False\n",
      "Step: 70, Reward: -0.034275785088539124, Done: False\n",
      "Step: 71, Reward: -0.041135288774967194, Done: False\n",
      "Step: 72, Reward: -0.05483432486653328, Done: False\n",
      "Step: 73, Reward: -0.06870419532060623, Done: False\n",
      "Step: 74, Reward: -0.07856140285730362, Done: False\n",
      "Step: 75, Reward: -0.09592228382825851, Done: False\n",
      "Step: 76, Reward: -0.11512172222137451, Done: False\n",
      "Step: 77, Reward: -0.13998401165008545, Done: False\n",
      "Step: 78, Reward: -0.17100831866264343, Done: False\n",
      "Step: 79, Reward: -0.20298364758491516, Done: False\n",
      "Step: 80, Reward: -0.2300109714269638, Done: False\n",
      "Step: 81, Reward: -0.2493593990802765, Done: False\n",
      "Step: 82, Reward: -0.270461767911911, Done: False\n",
      "Step: 83, Reward: -0.2978604733943939, Done: False\n",
      "Step: 84, Reward: -0.2987152338027954, Done: False\n",
      "Step: 85, Reward: -0.2824426293373108, Done: False\n",
      "Step: 86, Reward: -0.24235668778419495, Done: False\n",
      "Step: 87, Reward: -0.21809306740760803, Done: False\n",
      "Step: 88, Reward: -0.18367159366607666, Done: False\n",
      "Step: 89, Reward: -0.14396297931671143, Done: False\n",
      "Step: 90, Reward: -0.13870611786842346, Done: False\n",
      "Step: 91, Reward: -0.1645374596118927, Done: False\n",
      "Step: 92, Reward: -0.22078123688697815, Done: False\n",
      "Step: 93, Reward: -0.2410343438386917, Done: False\n",
      "Step: 94, Reward: -0.2876853942871094, Done: False\n",
      "Step: 95, Reward: -0.35350462794303894, Done: False\n",
      "Step: 96, Reward: -0.32325780391693115, Done: False\n",
      "Step: 97, Reward: -0.3053625524044037, Done: False\n",
      "Step: 98, Reward: -0.26957792043685913, Done: False\n",
      "Step: 99, Reward: -0.2417626976966858, Done: False\n",
      "Step: 100, Reward: -0.1867947280406952, Done: False\n",
      "Step: 101, Reward: -0.1620047390460968, Done: False\n",
      "Step: 102, Reward: -0.18285609781742096, Done: False\n",
      "Step: 103, Reward: -0.21462753415107727, Done: False\n",
      "Step: 104, Reward: -0.25402212142944336, Done: False\n",
      "Step: 105, Reward: -0.2943580150604248, Done: False\n",
      "Step: 106, Reward: -0.37207671999931335, Done: False\n",
      "Step: 107, Reward: -0.30858850479125977, Done: False\n",
      "Step: 108, Reward: -0.311519980430603, Done: False\n",
      "Step: 109, Reward: -0.2987118065357208, Done: False\n",
      "Step: 110, Reward: -0.2442394495010376, Done: False\n",
      "Step: 111, Reward: -0.22049537301063538, Done: False\n",
      "Step: 112, Reward: -0.20316094160079956, Done: False\n",
      "Step: 113, Reward: -0.15023434162139893, Done: False\n",
      "Step: 114, Reward: -0.07972601056098938, Done: False\n",
      "Step: 115, Reward: -0.107988640666008, Done: False\n",
      "Step: 116, Reward: -0.14389541745185852, Done: False\n",
      "Step: 117, Reward: -0.1879289746284485, Done: False\n",
      "Step: 118, Reward: -0.1995602250099182, Done: False\n",
      "Step: 119, Reward: -0.21348825097084045, Done: False\n",
      "Step: 120, Reward: -0.23144462704658508, Done: False\n",
      "Step: 121, Reward: -0.24953781068325043, Done: False\n",
      "Step: 122, Reward: -0.26996317505836487, Done: False\n",
      "Step: 123, Reward: -0.29679107666015625, Done: False\n",
      "Step: 124, Reward: -0.3250550925731659, Done: False\n",
      "Step: 125, Reward: -0.34837237000465393, Done: False\n",
      "Step: 126, Reward: -0.3656882047653198, Done: False\n",
      "Step: 127, Reward: -0.3040549159049988, Done: False\n",
      "Step: 128, Reward: -0.2474711537361145, Done: False\n",
      "Step: 129, Reward: -0.2575801908969879, Done: False\n",
      "Step: 130, Reward: -0.1990964710712433, Done: False\n",
      "Step: 131, Reward: -0.22734355926513672, Done: False\n",
      "Step: 132, Reward: -0.2715740203857422, Done: False\n",
      "Step: 133, Reward: -0.29632675647735596, Done: False\n",
      "Step: 134, Reward: -0.32993876934051514, Done: False\n",
      "Step: 135, Reward: -0.34130528569221497, Done: False\n",
      "Step: 136, Reward: -0.3529932200908661, Done: False\n",
      "Step: 137, Reward: -0.3100479245185852, Done: False\n",
      "Step: 138, Reward: -0.37324056029319763, Done: False\n",
      "Step: 139, Reward: -0.36100703477859497, Done: False\n",
      "Step: 140, Reward: -0.3343614339828491, Done: False\n",
      "Step: 141, Reward: -0.3121525049209595, Done: False\n",
      "Step: 142, Reward: -0.294322669506073, Done: False\n",
      "Step: 143, Reward: -0.2685282826423645, Done: False\n",
      "Step: 144, Reward: -0.2401544749736786, Done: False\n",
      "Step: 145, Reward: -0.22567608952522278, Done: False\n",
      "Step: 146, Reward: -0.1962357610464096, Done: False\n",
      "Step: 147, Reward: -0.17414265871047974, Done: False\n",
      "Step: 148, Reward: -0.2045675367116928, Done: False\n",
      "Step: 149, Reward: -0.2467663288116455, Done: False\n",
      "Step: 150, Reward: -0.28233465552330017, Done: False\n",
      "Step: 151, Reward: -0.31767725944519043, Done: False\n",
      "Step: 152, Reward: -0.3917711079120636, Done: False\n",
      "Step: 153, Reward: -0.3459817171096802, Done: False\n",
      "Step: 154, Reward: -0.332690566778183, Done: False\n",
      "Step: 155, Reward: -0.29633641242980957, Done: False\n",
      "Step: 156, Reward: -0.26802223920822144, Done: False\n",
      "Step: 157, Reward: -0.22891706228256226, Done: False\n",
      "Step 158: OUT OF BOUNDS!\n",
      "Step: 158, Reward: -50.1930046081543, Done: True\n",
      "Drone starting at: (22.33, 38.11) m\n",
      "Environment Reset. Wind Dir: 41.9 deg. Moving Platform: True\n",
      "Step: 159, Reward: 0.15850724279880524, Done: False\n",
      "Step: 160, Reward: 0.15903155505657196, Done: False\n",
      "Step: 161, Reward: 0.15962044894695282, Done: False\n",
      "Step: 162, Reward: 0.15683475136756897, Done: False\n",
      "Step: 163, Reward: 0.15894056856632233, Done: False\n",
      "Step: 164, Reward: 0.15853819251060486, Done: False\n",
      "Step: 165, Reward: 0.15994150936603546, Done: False\n",
      "Step: 166, Reward: 0.15630210936069489, Done: False\n",
      "Step: 167, Reward: 0.1542075127363205, Done: False\n",
      "Step: 168, Reward: 0.1518036425113678, Done: False\n",
      "Step: 169, Reward: 0.1529821902513504, Done: False\n",
      "Step: 170, Reward: 0.1527433693408966, Done: False\n",
      "Step: 171, Reward: 0.14946822822093964, Done: False\n",
      "Step: 172, Reward: 0.14918334782123566, Done: False\n",
      "Step: 173, Reward: 0.14669890701770782, Done: False\n",
      "Step: 174, Reward: 0.1443341225385666, Done: False\n",
      "Step: 175, Reward: 0.1398630291223526, Done: False\n",
      "Step: 176, Reward: 0.1406174600124359, Done: False\n",
      "Step: 177, Reward: 0.1423911452293396, Done: False\n",
      "Step: 178, Reward: 0.14187127351760864, Done: False\n",
      "Step: 179, Reward: 0.13817229866981506, Done: False\n",
      "Step: 180, Reward: 0.13452306389808655, Done: False\n",
      "Step: 181, Reward: 0.13649176061153412, Done: False\n",
      "Step: 182, Reward: 0.1390334963798523, Done: False\n",
      "Step: 183, Reward: 0.13630932569503784, Done: False\n",
      "Step: 184, Reward: 0.13756878674030304, Done: False\n",
      "Step: 185, Reward: 0.13859210908412933, Done: False\n",
      "Step: 186, Reward: 0.13437595963478088, Done: False\n",
      "Step: 187, Reward: 0.13382546603679657, Done: False\n",
      "Step: 188, Reward: 0.1270110011100769, Done: False\n",
      "Step: 189, Reward: 0.1255093365907669, Done: False\n",
      "Step: 190, Reward: 0.12300712615251541, Done: False\n",
      "Step: 191, Reward: 0.12339513003826141, Done: False\n",
      "Step: 192, Reward: 0.1279715597629547, Done: False\n",
      "Step: 193, Reward: 0.13373906910419464, Done: False\n",
      "Step: 194, Reward: 0.13539846241474152, Done: False\n",
      "Step: 195, Reward: 0.13528619706630707, Done: False\n",
      "Step: 196, Reward: 0.1356898546218872, Done: False\n",
      "Step: 197, Reward: 0.12937267124652863, Done: False\n",
      "Step: 198, Reward: 0.1317296177148819, Done: False\n",
      "Step: 199, Reward: 0.12960761785507202, Done: False\n",
      "Step: 200, Reward: 0.12491780519485474, Done: False\n",
      "Step: 201, Reward: 0.12151321768760681, Done: False\n",
      "Step: 202, Reward: 0.11655153334140778, Done: False\n",
      "Step: 203, Reward: 0.11951830983161926, Done: False\n",
      "Step: 204, Reward: 0.12085780501365662, Done: False\n",
      "Step: 205, Reward: 0.11962417513132095, Done: False\n",
      "Step: 206, Reward: 0.11592907458543777, Done: False\n",
      "Step: 207, Reward: 0.11844833195209503, Done: False\n",
      "Step: 208, Reward: 0.11656264960765839, Done: False\n",
      "Step: 209, Reward: 0.11653939634561539, Done: False\n",
      "Step: 210, Reward: 0.11450785398483276, Done: False\n",
      "Step: 211, Reward: 0.10897176712751389, Done: False\n",
      "Step: 212, Reward: 0.10136448591947556, Done: False\n",
      "Step: 213, Reward: 0.10536583513021469, Done: False\n",
      "Step: 214, Reward: 0.10020440071821213, Done: False\n",
      "Step: 215, Reward: 0.09967611730098724, Done: False\n",
      "Step: 216, Reward: 0.09948702156543732, Done: False\n",
      "Step: 217, Reward: 0.10038602352142334, Done: False\n",
      "Step: 218, Reward: 0.10048594325780869, Done: False\n",
      "Step: 219, Reward: 0.10308881103992462, Done: False\n",
      "Step: 220, Reward: 0.1006370484828949, Done: False\n",
      "Step: 221, Reward: 0.09793547540903091, Done: False\n",
      "Step: 222, Reward: 0.09799453616142273, Done: False\n",
      "Step: 223, Reward: 0.09834311157464981, Done: False\n",
      "Step: 224, Reward: 0.09134102612733841, Done: False\n",
      "Step: 225, Reward: 0.09006094932556152, Done: False\n",
      "Step: 226, Reward: 0.0903908833861351, Done: False\n",
      "Step: 227, Reward: 0.09300138056278229, Done: False\n",
      "Step: 228, Reward: 0.08778577297925949, Done: False\n",
      "Step: 229, Reward: 0.09127310663461685, Done: False\n",
      "Step: 230, Reward: 0.08942893147468567, Done: False\n",
      "Step: 231, Reward: 0.0854242816567421, Done: False\n",
      "Step: 232, Reward: 0.07897411286830902, Done: False\n",
      "Step: 233, Reward: 0.07189024239778519, Done: False\n",
      "Step: 234, Reward: 0.07035582512617111, Done: False\n",
      "Step: 235, Reward: 0.07207495719194412, Done: False\n",
      "Step: 236, Reward: 0.06726862490177155, Done: False\n",
      "Step: 237, Reward: 0.06253179162740707, Done: False\n",
      "Step: 238, Reward: 0.05781332030892372, Done: False\n",
      "Step: 239, Reward: 0.05670153349637985, Done: False\n",
      "Step: 240, Reward: 0.053820401430130005, Done: False\n",
      "Step: 241, Reward: 0.04345950856804848, Done: False\n",
      "Step: 242, Reward: 0.034417130053043365, Done: False\n",
      "Step: 243, Reward: 0.02678041160106659, Done: False\n",
      "Step: 244, Reward: 0.021268609911203384, Done: False\n",
      "Step: 245, Reward: 0.010993964970111847, Done: False\n",
      "Step: 246, Reward: 0.0037681199610233307, Done: False\n",
      "Step: 247, Reward: -0.0022270716726779938, Done: False\n",
      "Step: 248, Reward: -0.0034735575318336487, Done: False\n",
      "Step: 249, Reward: -0.004063766449689865, Done: False\n",
      "Step: 250, Reward: -0.012335501611232758, Done: False\n",
      "Step: 251, Reward: -0.017801225185394287, Done: False\n",
      "Step: 252, Reward: -0.020041123032569885, Done: False\n",
      "Step: 253, Reward: -0.027201563119888306, Done: False\n",
      "Step: 254, Reward: -0.04030464589595795, Done: False\n",
      "Step: 255, Reward: -0.04707929491996765, Done: False\n",
      "Step: 256, Reward: -0.06219688802957535, Done: False\n",
      "Step 98: OUT OF BOUNDS!\n",
      "Step: 257, Reward: -50.074100494384766, Done: True\n",
      "Drone starting at: (22.87, 42.67) m\n",
      "Environment Reset. Wind Dir: 310.7 deg. Moving Platform: True\n",
      "Step: 258, Reward: 0.16007322072982788, Done: False\n",
      "Step: 259, Reward: 0.15532349050045013, Done: False\n",
      "Step: 260, Reward: 0.14652639627456665, Done: False\n",
      "Step: 261, Reward: 0.1437951624393463, Done: False\n",
      "Step: 262, Reward: 0.1428116261959076, Done: False\n",
      "Step: 263, Reward: 0.13831326365470886, Done: False\n",
      "Step: 264, Reward: 0.13326287269592285, Done: False\n",
      "Step: 265, Reward: 0.13274772465229034, Done: False\n",
      "Step: 266, Reward: 0.13099953532218933, Done: False\n",
      "Step: 267, Reward: 0.12738899886608124, Done: False\n",
      "Step: 268, Reward: 0.12207217514514923, Done: False\n",
      "Step: 269, Reward: 0.12268417328596115, Done: False\n",
      "Step: 270, Reward: 0.12223348766565323, Done: False\n",
      "Step: 271, Reward: 0.11683826148509979, Done: False\n",
      "Step: 272, Reward: 0.12132574617862701, Done: False\n",
      "Step: 273, Reward: 0.12051050364971161, Done: False\n",
      "Step: 274, Reward: 0.12442325055599213, Done: False\n",
      "Step: 275, Reward: 0.12270176410675049, Done: False\n",
      "Step: 276, Reward: 0.12185351550579071, Done: False\n",
      "Step: 277, Reward: 0.11836555600166321, Done: False\n",
      "Step: 278, Reward: 0.11179368942975998, Done: False\n",
      "Step: 279, Reward: 0.11370676010847092, Done: False\n",
      "Step: 280, Reward: 0.11278335750102997, Done: False\n",
      "Step: 281, Reward: 0.11295700073242188, Done: False\n",
      "Step: 282, Reward: 0.11405181139707565, Done: False\n",
      "Step: 283, Reward: 0.1153656467795372, Done: False\n",
      "Step: 284, Reward: 0.11961114406585693, Done: False\n",
      "Step: 285, Reward: 0.12196341156959534, Done: False\n",
      "Step: 286, Reward: 0.12576347589492798, Done: False\n",
      "Step: 287, Reward: 0.12469859421253204, Done: False\n",
      "Step: 288, Reward: 0.1265498697757721, Done: False\n",
      "Step: 289, Reward: 0.12732531130313873, Done: False\n",
      "Step: 290, Reward: 0.13288988173007965, Done: False\n",
      "Step: 291, Reward: 0.1331554502248764, Done: False\n",
      "Step: 292, Reward: 0.13685418665409088, Done: False\n",
      "Step: 293, Reward: 0.13544535636901855, Done: False\n",
      "Step: 294, Reward: 0.13523656129837036, Done: False\n",
      "Step: 295, Reward: 0.1346539705991745, Done: False\n",
      "Step: 296, Reward: 0.13008750975131989, Done: False\n",
      "Step: 297, Reward: 0.1322084218263626, Done: False\n",
      "Step: 298, Reward: 0.1277104765176773, Done: False\n",
      "Step: 299, Reward: 0.12374987453222275, Done: False\n",
      "Step: 300, Reward: 0.12406021356582642, Done: False\n",
      "Step: 301, Reward: 0.11390796303749084, Done: False\n",
      "Step: 302, Reward: 0.11109020560979843, Done: False\n",
      "Step: 303, Reward: 0.10949596762657166, Done: False\n",
      "Step: 304, Reward: 0.10626193135976791, Done: False\n",
      "Step: 305, Reward: 0.10146251320838928, Done: False\n",
      "Step: 306, Reward: 0.09527061134576797, Done: False\n",
      "Step: 307, Reward: 0.09260367602109909, Done: False\n",
      "Step: 308, Reward: 0.09118860960006714, Done: False\n",
      "Step: 309, Reward: 0.08344390988349915, Done: False\n",
      "Step: 310, Reward: 0.08259961009025574, Done: False\n",
      "Step: 311, Reward: 0.07954742014408112, Done: False\n",
      "Step: 312, Reward: 0.07682820409536362, Done: False\n",
      "Step: 313, Reward: 0.0744786486029625, Done: False\n",
      "Step: 314, Reward: 0.06329655647277832, Done: False\n",
      "Step: 315, Reward: 0.05679646134376526, Done: False\n",
      "Step: 316, Reward: 0.053766652941703796, Done: False\n",
      "Step: 317, Reward: 0.04722856730222702, Done: False\n",
      "Step: 318, Reward: 0.040683843195438385, Done: False\n",
      "Step: 319, Reward: 0.030765414237976074, Done: False\n",
      "Step: 320, Reward: 0.016434140503406525, Done: False\n",
      "Step: 321, Reward: 0.004436902701854706, Done: False\n",
      "Step: 322, Reward: -0.003556281328201294, Done: False\n",
      "Step: 323, Reward: -0.012059681117534637, Done: False\n",
      "Step: 324, Reward: -0.026878684759140015, Done: False\n",
      "Step: 325, Reward: -0.04598689079284668, Done: False\n",
      "Step: 326, Reward: -0.06772499531507492, Done: False\n",
      "Step: 327, Reward: -0.0908258706331253, Done: False\n",
      "Step: 328, Reward: -0.11313547939062119, Done: False\n",
      "Step: 329, Reward: -0.12876363098621368, Done: False\n",
      "Step: 330, Reward: -0.1485115885734558, Done: False\n",
      "Step: 331, Reward: -0.16839860379695892, Done: False\n",
      "Step: 332, Reward: -0.1934444010257721, Done: False\n",
      "Step: 333, Reward: -0.18715094029903412, Done: False\n",
      "Step: 334, Reward: -0.16922608017921448, Done: False\n",
      "Step: 335, Reward: -0.1402159333229065, Done: False\n",
      "Step: 336, Reward: -0.10907422751188278, Done: False\n",
      "Step: 337, Reward: -0.08046708256006241, Done: False\n",
      "Step: 338, Reward: -0.0642293393611908, Done: False\n",
      "Step: 339, Reward: -0.028919532895088196, Done: False\n",
      "Step: 340, Reward: -0.012832418084144592, Done: False\n",
      "Step: 341, Reward: -0.020789965987205505, Done: False\n",
      "Step: 342, Reward: 0.027483362704515457, Done: False\n",
      "Step: 343, Reward: 0.041542667895555496, Done: False\n",
      "Step: 344, Reward: 0.04494558647274971, Done: False\n",
      "Step: 345, Reward: 0.032620739191770554, Done: False\n",
      "Step: 346, Reward: 0.017704516649246216, Done: False\n",
      "Step: 347, Reward: 0.016071796417236328, Done: False\n",
      "Step: 348, Reward: 0.034687139093875885, Done: False\n",
      "Step: 349, Reward: 0.04384717345237732, Done: False\n",
      "Step: 350, Reward: 0.032816704362630844, Done: False\n",
      "Step: 351, Reward: 0.002865239977836609, Done: False\n",
      "Step: 352, Reward: 0.016361456364393234, Done: False\n",
      "Step: 353, Reward: 0.01250266283750534, Done: False\n",
      "Step: 354, Reward: -0.009880736470222473, Done: False\n",
      "Step: 355, Reward: -0.03833683580160141, Done: False\n",
      "Step: 356, Reward: -0.06036490201950073, Done: False\n",
      "Step: 357, Reward: -0.08017931133508682, Done: False\n",
      "Step: 358, Reward: -0.10167820751667023, Done: False\n",
      "Step: 359, Reward: -0.13386797904968262, Done: False\n",
      "Step: 360, Reward: -0.16879084706306458, Done: False\n",
      "Step: 361, Reward: -0.19553181529045105, Done: False\n",
      "Step: 362, Reward: -0.22911299765110016, Done: False\n",
      "Step: 363, Reward: -0.2041158527135849, Done: False\n",
      "Step: 364, Reward: -0.156768336892128, Done: False\n",
      "Step: 365, Reward: -0.13797205686569214, Done: False\n",
      "Step: 366, Reward: -0.1242503747344017, Done: False\n",
      "Step: 367, Reward: -0.1117134541273117, Done: False\n",
      "Step: 368, Reward: -0.08827318251132965, Done: False\n",
      "Step: 369, Reward: -0.08371763676404953, Done: False\n",
      "Step: 370, Reward: -0.06952965259552002, Done: False\n",
      "Step: 371, Reward: -0.0532631017267704, Done: False\n",
      "Step: 372, Reward: -0.05820206552743912, Done: False\n",
      "Step: 373, Reward: -0.08668224513530731, Done: False\n",
      "Step: 374, Reward: -0.12259980291128159, Done: False\n",
      "Step: 375, Reward: -0.17187248170375824, Done: False\n",
      "Step: 376, Reward: -0.22409985959529877, Done: False\n",
      "Step: 377, Reward: -0.23838385939598083, Done: False\n",
      "Step: 378, Reward: -0.2235548496246338, Done: False\n",
      "Step: 379, Reward: -0.20357581973075867, Done: False\n",
      "Step: 380, Reward: -0.1670529693365097, Done: False\n",
      "Step: 381, Reward: -0.13838742673397064, Done: False\n",
      "Step: 382, Reward: -0.10209131240844727, Done: False\n",
      "Step: 383, Reward: -0.07450985908508301, Done: False\n",
      "Step: 384, Reward: -0.08194772154092789, Done: False\n",
      "Step: 385, Reward: -0.11871228367090225, Done: False\n",
      "Step: 386, Reward: -0.21212847530841827, Done: False\n",
      "Step: 387, Reward: -0.20865671336650848, Done: False\n",
      "Step: 388, Reward: -0.2336248904466629, Done: False\n",
      "Step: 389, Reward: -0.25611844658851624, Done: False\n",
      "Step: 390, Reward: -0.2594856023788452, Done: False\n",
      "Step: 391, Reward: -0.23420633375644684, Done: False\n",
      "Step: 392, Reward: -0.21741801500320435, Done: False\n",
      "Step: 393, Reward: -0.12967544794082642, Done: False\n",
      "Step: 394, Reward: -0.12076550722122192, Done: False\n",
      "Step: 395, Reward: -0.12174947559833527, Done: False\n",
      "Step: 396, Reward: -0.15391957759857178, Done: False\n",
      "Step: 397, Reward: -0.17916378378868103, Done: False\n",
      "Step: 398, Reward: -0.1912054717540741, Done: False\n",
      "Step: 399, Reward: -0.20718249678611755, Done: False\n",
      "Step: 400, Reward: -0.2298126220703125, Done: False\n",
      "Step: 401, Reward: -0.24759015440940857, Done: False\n",
      "Step: 402, Reward: -0.2598709464073181, Done: False\n",
      "Step: 403, Reward: -0.2788349986076355, Done: False\n",
      "Step: 404, Reward: -0.2819535732269287, Done: False\n",
      "Step: 405, Reward: -0.2948019802570343, Done: False\n",
      "Step: 406, Reward: -0.2846144139766693, Done: False\n",
      "Step: 407, Reward: -0.2655077576637268, Done: False\n",
      "Step: 408, Reward: -0.2547845244407654, Done: False\n",
      "Step: 409, Reward: -0.2495914101600647, Done: False\n",
      "Step: 410, Reward: -0.2428901195526123, Done: False\n",
      "Step: 411, Reward: -0.23488549888134003, Done: False\n",
      "Step: 412, Reward: -0.2269776165485382, Done: False\n",
      "Step: 413, Reward: -0.2371879369020462, Done: False\n",
      "Step: 414, Reward: -0.2343289852142334, Done: False\n",
      "Step: 415, Reward: -0.23561111092567444, Done: False\n",
      "Step: 416, Reward: -0.23716086149215698, Done: False\n",
      "Step: 417, Reward: -0.2388896942138672, Done: False\n",
      "Step: 418, Reward: -0.24439737200737, Done: False\n",
      "Step: 419, Reward: -0.24770233035087585, Done: False\n",
      "Step: 420, Reward: -0.25274908542633057, Done: False\n",
      "Step: 421, Reward: -0.2601170837879181, Done: False\n",
      "Step: 422, Reward: -0.25796231627464294, Done: False\n",
      "Step: 423, Reward: -0.26533615589141846, Done: False\n",
      "Step: 424, Reward: -0.259834885597229, Done: False\n",
      "Step: 425, Reward: -0.2644304633140564, Done: False\n",
      "Step: 426, Reward: -0.25175386667251587, Done: False\n",
      "Step: 427, Reward: -0.235499769449234, Done: False\n",
      "Step: 428, Reward: -0.2207653522491455, Done: False\n",
      "Step: 429, Reward: -0.20498204231262207, Done: False\n",
      "Step: 430, Reward: -0.18868255615234375, Done: False\n",
      "Step: 431, Reward: -0.17200274765491486, Done: False\n",
      "Step: 432, Reward: -0.1544783115386963, Done: False\n",
      "Step: 433, Reward: -0.13596594333648682, Done: False\n",
      "Step: 434, Reward: -0.11543463915586472, Done: False\n",
      "Step: 435, Reward: -0.1302318572998047, Done: False\n",
      "Step: 436, Reward: -0.1577928364276886, Done: False\n",
      "Step: 437, Reward: -0.18668529391288757, Done: False\n",
      "Step: 438, Reward: -0.2144138216972351, Done: False\n",
      "Step: 439, Reward: -0.24856024980545044, Done: False\n",
      "Step: 440, Reward: -0.2736663818359375, Done: False\n",
      "Step: 441, Reward: -0.27566736936569214, Done: False\n",
      "Step: 442, Reward: -0.24061384797096252, Done: False\n",
      "Step: 443, Reward: -0.20772433280944824, Done: False\n",
      "Step: 444, Reward: -0.17342153191566467, Done: False\n",
      "Step: 445, Reward: -0.1324978917837143, Done: False\n",
      "Step: 446, Reward: -0.09932777285575867, Done: False\n",
      "Step: 447, Reward: -0.06908050179481506, Done: False\n",
      "Step: 448, Reward: -0.046365030109882355, Done: False\n",
      "Step: 449, Reward: -0.06345269829034805, Done: False\n",
      "Step: 450, Reward: -0.08110981434583664, Done: False\n",
      "Step: 451, Reward: -0.09303255379199982, Done: False\n",
      "Step: 452, Reward: -0.1058039739727974, Done: False\n",
      "Step: 453, Reward: -0.12483471632003784, Done: False\n",
      "Step: 454, Reward: -0.14020371437072754, Done: False\n",
      "Step: 455, Reward: -0.1558893322944641, Done: False\n",
      "Step: 456, Reward: -0.17177414894104004, Done: False\n",
      "Step: 457, Reward: -0.16919997334480286, Done: False\n",
      "Step: 458, Reward: -0.14094915986061096, Done: False\n",
      "Step: 459, Reward: -0.11389397084712982, Done: False\n",
      "Step: 460, Reward: -0.08551278710365295, Done: False\n",
      "Step: 461, Reward: -0.05677378177642822, Done: False\n",
      "Step: 462, Reward: -0.025021851062774658, Done: False\n",
      "Step: 463, Reward: 0.008500441908836365, Done: False\n",
      "Step: 464, Reward: 0.041961051523685455, Done: False\n",
      "Step 207: CRASHED on ground!\n",
      "Step: 465, Reward: -49.983375549316406, Done: True\n",
      "Drone starting at: (24.20, 42.74) m\n",
      "Environment Reset. Wind Dir: 60.6 deg. Moving Platform: True\n",
      "Step: 466, Reward: 0.15863652527332306, Done: False\n",
      "Step: 467, Reward: 0.15843957662582397, Done: False\n",
      "Step: 468, Reward: 0.15589891374111176, Done: False\n",
      "Step: 469, Reward: 0.15657751262187958, Done: False\n",
      "Step: 470, Reward: 0.15781787037849426, Done: False\n",
      "Step: 471, Reward: 0.1527741253376007, Done: False\n",
      "Step: 472, Reward: 0.1568753719329834, Done: False\n",
      "Step: 473, Reward: 0.160051628947258, Done: False\n",
      "Step: 474, Reward: 0.15753407776355743, Done: False\n",
      "Step: 475, Reward: 0.15868520736694336, Done: False\n",
      "Step: 476, Reward: 0.1537780910730362, Done: False\n",
      "Step: 477, Reward: 0.14914293587207794, Done: False\n",
      "Step: 478, Reward: 0.1426643431186676, Done: False\n",
      "Step: 479, Reward: 0.13607196509838104, Done: False\n",
      "Step: 480, Reward: 0.13632266223430634, Done: False\n",
      "Step: 481, Reward: 0.13564631342887878, Done: False\n",
      "Step: 482, Reward: 0.13738752901554108, Done: False\n",
      "Step: 483, Reward: 0.14035938680171967, Done: False\n",
      "Step: 484, Reward: 0.14334355294704437, Done: False\n",
      "Step: 485, Reward: 0.13635632395744324, Done: False\n",
      "Step: 486, Reward: 0.1310572773218155, Done: False\n",
      "Step: 487, Reward: 0.1308126151561737, Done: False\n",
      "Step: 488, Reward: 0.1353670060634613, Done: False\n",
      "Step: 489, Reward: 0.1383969783782959, Done: False\n",
      "Step: 490, Reward: 0.1378331184387207, Done: False\n",
      "Step: 491, Reward: 0.13696952164173126, Done: False\n",
      "Step: 492, Reward: 0.1392545849084854, Done: False\n",
      "Step: 493, Reward: 0.13534344732761383, Done: False\n",
      "Step: 494, Reward: 0.1339590847492218, Done: False\n",
      "Step: 495, Reward: 0.13704335689544678, Done: False\n",
      "Step: 496, Reward: 0.13832706212997437, Done: False\n",
      "Step: 497, Reward: 0.13682422041893005, Done: False\n",
      "Step: 498, Reward: 0.13341142237186432, Done: False\n",
      "Step: 499, Reward: 0.13596323132514954, Done: False\n",
      "Step: 500, Reward: 0.1385568082332611, Done: False\n",
      "Step: 501, Reward: 0.1361226737499237, Done: False\n",
      "Step: 502, Reward: 0.13667888939380646, Done: False\n",
      "Step: 503, Reward: 0.13236713409423828, Done: False\n",
      "Step: 504, Reward: 0.13290996849536896, Done: False\n",
      "Step: 505, Reward: 0.134238138794899, Done: False\n",
      "Step: 506, Reward: 0.1314787119626999, Done: False\n",
      "Step: 507, Reward: 0.12435222417116165, Done: False\n",
      "Step: 508, Reward: 0.12215681374073029, Done: False\n",
      "Step: 509, Reward: 0.11519480496644974, Done: False\n",
      "Step: 510, Reward: 0.11151637881994247, Done: False\n",
      "Step: 511, Reward: 0.11041787266731262, Done: False\n",
      "Step: 512, Reward: 0.10814552009105682, Done: False\n",
      "Step: 513, Reward: 0.10262247920036316, Done: False\n",
      "Step: 514, Reward: 0.10261362791061401, Done: False\n",
      "Step: 515, Reward: 0.09999748319387436, Done: False\n",
      "Step: 516, Reward: 0.09576919674873352, Done: False\n",
      "Step: 517, Reward: 0.09681057929992676, Done: False\n",
      "Step: 518, Reward: 0.0981626883149147, Done: False\n",
      "Step: 519, Reward: 0.09675620496273041, Done: False\n",
      "Step: 520, Reward: 0.09572757035493851, Done: False\n",
      "Step: 521, Reward: 0.09404567629098892, Done: False\n",
      "Step: 522, Reward: 0.0878937840461731, Done: False\n",
      "Step: 523, Reward: 0.09094444662332535, Done: False\n",
      "Step: 524, Reward: 0.08676150441169739, Done: False\n",
      "Step: 525, Reward: 0.07995477318763733, Done: False\n",
      "Step: 526, Reward: 0.07525528967380524, Done: False\n",
      "Step: 527, Reward: 0.07694442570209503, Done: False\n",
      "Step: 528, Reward: 0.07207576185464859, Done: False\n",
      "Step: 529, Reward: 0.07491359859704971, Done: False\n",
      "Step: 530, Reward: 0.07388115674257278, Done: False\n",
      "Step: 531, Reward: 0.07264573127031326, Done: False\n",
      "Step: 532, Reward: 0.06526350229978561, Done: False\n",
      "Step: 533, Reward: 0.06465576589107513, Done: False\n",
      "Step: 534, Reward: 0.06368307769298553, Done: False\n",
      "Step: 535, Reward: 0.06464125961065292, Done: False\n",
      "Step: 536, Reward: 0.06100745499134064, Done: False\n",
      "Step: 537, Reward: 0.05395897105336189, Done: False\n",
      "Step: 538, Reward: 0.04832286015152931, Done: False\n",
      "Step: 539, Reward: 0.042814891785383224, Done: False\n",
      "Step 74: OUT OF BOUNDS!\n",
      "Step: 540, Reward: -49.95885467529297, Done: True\n",
      "Drone starting at: (27.74, 37.05) m\n",
      "Environment Reset. Wind Dir: 38.1 deg. Moving Platform: True\n",
      "Step: 541, Reward: 0.15866808593273163, Done: False\n",
      "Step: 542, Reward: 0.15685883164405823, Done: False\n",
      "Step: 543, Reward: 0.1579815000295639, Done: False\n",
      "Step: 544, Reward: 0.16045093536376953, Done: False\n",
      "Step: 545, Reward: 0.16001969575881958, Done: False\n",
      "Step: 546, Reward: 0.16125749051570892, Done: False\n",
      "Step: 547, Reward: 0.1569843590259552, Done: False\n",
      "Step: 548, Reward: 0.15718123316764832, Done: False\n",
      "Step: 549, Reward: 0.15524418652057648, Done: False\n",
      "Step: 550, Reward: 0.1558482050895691, Done: False\n",
      "Step: 551, Reward: 0.1527814269065857, Done: False\n",
      "Step: 552, Reward: 0.1556529402732849, Done: False\n",
      "Step: 553, Reward: 0.1574075073003769, Done: False\n",
      "Step: 554, Reward: 0.15256986021995544, Done: False\n",
      "Step: 555, Reward: 0.15335196256637573, Done: False\n",
      "Step: 556, Reward: 0.15642648935317993, Done: False\n",
      "Step: 557, Reward: 0.15346543490886688, Done: False\n",
      "Step: 558, Reward: 0.15037059783935547, Done: False\n",
      "Step: 559, Reward: 0.15076524019241333, Done: False\n",
      "Step: 560, Reward: 0.14687977731227875, Done: False\n",
      "Step: 561, Reward: 0.15097743272781372, Done: False\n",
      "Step: 562, Reward: 0.1511959433555603, Done: False\n",
      "Step: 563, Reward: 0.1521482765674591, Done: False\n",
      "Step: 564, Reward: 0.14793488383293152, Done: False\n",
      "Step: 565, Reward: 0.14658088982105255, Done: False\n",
      "Step: 566, Reward: 0.15084297955036163, Done: False\n",
      "Step: 567, Reward: 0.14881286025047302, Done: False\n",
      "Step: 568, Reward: 0.15153907239437103, Done: False\n",
      "Step: 569, Reward: 0.14982448518276215, Done: False\n",
      "Step: 570, Reward: 0.15258388221263885, Done: False\n",
      "Step: 571, Reward: 0.15158964693546295, Done: False\n",
      "Step: 572, Reward: 0.14451885223388672, Done: False\n",
      "Step: 573, Reward: 0.14264465868473053, Done: False\n",
      "Step: 574, Reward: 0.13725806772708893, Done: False\n",
      "Step: 575, Reward: 0.14168593287467957, Done: False\n",
      "Step: 576, Reward: 0.14443030953407288, Done: False\n",
      "Step: 577, Reward: 0.14472222328186035, Done: False\n",
      "Step: 578, Reward: 0.14484712481498718, Done: False\n",
      "Step: 579, Reward: 0.13645461201667786, Done: False\n",
      "Step: 580, Reward: 0.13762430846691132, Done: False\n",
      "Step: 581, Reward: 0.13621334731578827, Done: False\n",
      "Step: 582, Reward: 0.1368110030889511, Done: False\n",
      "Step: 583, Reward: 0.1379757523536682, Done: False\n",
      "Step: 584, Reward: 0.14192794263362885, Done: False\n",
      "Step: 585, Reward: 0.14489907026290894, Done: False\n",
      "Step: 586, Reward: 0.14317426085472107, Done: False\n",
      "Step: 587, Reward: 0.1444944441318512, Done: False\n",
      "Step: 588, Reward: 0.143599271774292, Done: False\n",
      "Step: 589, Reward: 0.13963325321674347, Done: False\n",
      "Step: 590, Reward: 0.1440649926662445, Done: False\n",
      "Step: 591, Reward: 0.14002974331378937, Done: False\n",
      "Step: 592, Reward: 0.13800787925720215, Done: False\n",
      "Step: 593, Reward: 0.13977187871932983, Done: False\n",
      "Step: 594, Reward: 0.13830211758613586, Done: False\n",
      "Step: 595, Reward: 0.13486561179161072, Done: False\n",
      "Step: 596, Reward: 0.1366715133190155, Done: False\n",
      "Step: 597, Reward: 0.13528592884540558, Done: False\n",
      "Step: 598, Reward: 0.13386543095111847, Done: False\n",
      "Step: 599, Reward: 0.13013915717601776, Done: False\n",
      "Step: 600, Reward: 0.13015395402908325, Done: False\n",
      "Step: 601, Reward: 0.12570539116859436, Done: False\n",
      "Step: 602, Reward: 0.11776629090309143, Done: False\n",
      "Step: 603, Reward: 0.1102999672293663, Done: False\n",
      "Step: 604, Reward: 0.11174609512090683, Done: False\n",
      "Step: 605, Reward: 0.11024446040391922, Done: False\n",
      "Step: 606, Reward: 0.11207802593708038, Done: False\n",
      "Step: 607, Reward: 0.1136077344417572, Done: False\n",
      "Step: 608, Reward: 0.11282645165920258, Done: False\n",
      "Step: 609, Reward: 0.10969379544258118, Done: False\n",
      "Step: 610, Reward: 0.11031687259674072, Done: False\n",
      "Step: 611, Reward: 0.10721679776906967, Done: False\n",
      "Step: 612, Reward: 0.10566962510347366, Done: False\n",
      "Step: 613, Reward: 0.10282061994075775, Done: False\n",
      "Step: 614, Reward: 0.10562571883201599, Done: False\n",
      "Step: 615, Reward: 0.10001210868358612, Done: False\n",
      "Step: 616, Reward: 0.09354464709758759, Done: False\n",
      "Step: 617, Reward: 0.0959358736872673, Done: False\n",
      "Step: 618, Reward: 0.09506818652153015, Done: False\n",
      "Step: 619, Reward: 0.08907370269298553, Done: False\n",
      "Step: 620, Reward: 0.09058337658643723, Done: False\n",
      "Step: 621, Reward: 0.09064310044050217, Done: False\n",
      "Step: 622, Reward: 0.08618354797363281, Done: False\n",
      "Step: 623, Reward: 0.08950727432966232, Done: False\n",
      "Step: 624, Reward: 0.08869540691375732, Done: False\n",
      "Step: 625, Reward: 0.0881100594997406, Done: False\n",
      "Step: 626, Reward: 0.08769162744283676, Done: False\n",
      "Step: 627, Reward: 0.08165162801742554, Done: False\n",
      "Step: 628, Reward: 0.08302013576030731, Done: False\n",
      "Step: 629, Reward: 0.07945498079061508, Done: False\n",
      "Step: 630, Reward: 0.07402454316616058, Done: False\n",
      "Step: 631, Reward: 0.07530449330806732, Done: False\n",
      "Step: 632, Reward: 0.07360053062438965, Done: False\n",
      "Step: 633, Reward: 0.06951965391635895, Done: False\n",
      "Step: 634, Reward: 0.06325876712799072, Done: False\n",
      "Step: 635, Reward: 0.06555065512657166, Done: False\n",
      "Step: 636, Reward: 0.05952619016170502, Done: False\n",
      "Step: 637, Reward: 0.0495469830930233, Done: False\n",
      "Step: 638, Reward: 0.04159488156437874, Done: False\n",
      "Step: 639, Reward: 0.042673856019973755, Done: False\n",
      "Step: 640, Reward: 0.04325122386217117, Done: False\n",
      "Step: 641, Reward: 0.03931868076324463, Done: False\n",
      "Step: 642, Reward: 0.03312864527106285, Done: False\n",
      "Step: 643, Reward: 0.03572256490588188, Done: False\n",
      "Step: 644, Reward: 0.03346388041973114, Done: False\n",
      "Step: 645, Reward: 0.035270825028419495, Done: False\n",
      "Step: 646, Reward: 0.030740678310394287, Done: False\n",
      "Step: 647, Reward: 0.02815372683107853, Done: False\n",
      "Step: 648, Reward: 0.029253166168928146, Done: False\n",
      "Step: 649, Reward: 0.03007691539824009, Done: False\n",
      "Step: 650, Reward: 0.029613612219691277, Done: False\n",
      "Step 110: OUT OF BOUNDS!\n",
      "Step: 651, Reward: -49.97077560424805, Done: True\n",
      "Drone starting at: (22.46, 37.38) m\n",
      "Environment Reset. Wind Dir: 1.6 deg. Moving Platform: True\n",
      "Step: 652, Reward: 0.15936288237571716, Done: False\n",
      "Step: 653, Reward: 0.15944266319274902, Done: False\n",
      "Step: 654, Reward: 0.15460053086280823, Done: False\n",
      "Step: 655, Reward: 0.14810092747211456, Done: False\n",
      "Step: 656, Reward: 0.14447857439517975, Done: False\n",
      "Step: 657, Reward: 0.13947290182113647, Done: False\n",
      "Step: 658, Reward: 0.1447240114212036, Done: False\n",
      "Step: 659, Reward: 0.14645640552043915, Done: False\n",
      "Step: 660, Reward: 0.1474158763885498, Done: False\n",
      "Step: 661, Reward: 0.14829301834106445, Done: False\n",
      "Step: 662, Reward: 0.14441320300102234, Done: False\n",
      "Step: 663, Reward: 0.14014969766139984, Done: False\n",
      "Step: 664, Reward: 0.14063896238803864, Done: False\n",
      "Step: 665, Reward: 0.1426011472940445, Done: False\n",
      "Step: 666, Reward: 0.14355380833148956, Done: False\n",
      "Step: 667, Reward: 0.14276690781116486, Done: False\n",
      "Step: 668, Reward: 0.14248725771903992, Done: False\n",
      "Step: 669, Reward: 0.1438698172569275, Done: False\n",
      "Step: 670, Reward: 0.13814334571361542, Done: False\n",
      "Step: 671, Reward: 0.13329872488975525, Done: False\n",
      "Step: 672, Reward: 0.13451598584651947, Done: False\n",
      "Step: 673, Reward: 0.1392601877450943, Done: False\n",
      "Step: 674, Reward: 0.13456614315509796, Done: False\n",
      "Step: 675, Reward: 0.13436268270015717, Done: False\n",
      "Step: 676, Reward: 0.14008823037147522, Done: False\n",
      "Step: 677, Reward: 0.1456049680709839, Done: False\n",
      "Step: 678, Reward: 0.15217764675617218, Done: False\n",
      "Step: 679, Reward: 0.15187405049800873, Done: False\n",
      "Step: 680, Reward: 0.1477551907300949, Done: False\n",
      "Step: 681, Reward: 0.154122456908226, Done: False\n",
      "Step: 682, Reward: 0.15486611425876617, Done: False\n",
      "Step: 683, Reward: 0.15671204030513763, Done: False\n",
      "Step: 684, Reward: 0.15433448553085327, Done: False\n",
      "Step: 685, Reward: 0.15167240798473358, Done: False\n",
      "Step: 686, Reward: 0.15519647300243378, Done: False\n",
      "Step: 687, Reward: 0.15561795234680176, Done: False\n",
      "Step: 688, Reward: 0.1569538414478302, Done: False\n",
      "Step: 689, Reward: 0.1546086221933365, Done: False\n",
      "Step: 690, Reward: 0.15367341041564941, Done: False\n",
      "Step: 691, Reward: 0.14478403329849243, Done: False\n",
      "Step: 692, Reward: 0.14266766607761383, Done: False\n",
      "Step: 693, Reward: 0.1401911973953247, Done: False\n",
      "Step: 694, Reward: 0.1354232281446457, Done: False\n",
      "Step: 695, Reward: 0.1317032277584076, Done: False\n",
      "Step: 696, Reward: 0.12802782654762268, Done: False\n",
      "Step: 697, Reward: 0.12803058326244354, Done: False\n",
      "Step: 698, Reward: 0.12553589046001434, Done: False\n",
      "Step: 699, Reward: 0.12664413452148438, Done: False\n",
      "Step: 700, Reward: 0.12966620922088623, Done: False\n",
      "Step: 701, Reward: 0.13153567910194397, Done: False\n",
      "Step: 702, Reward: 0.12906289100646973, Done: False\n",
      "Step: 703, Reward: 0.1296691596508026, Done: False\n",
      "Step: 704, Reward: 0.12520627677440643, Done: False\n",
      "Step: 705, Reward: 0.12459592521190643, Done: False\n",
      "Step: 706, Reward: 0.1260911375284195, Done: False\n",
      "Step: 707, Reward: 0.12527836859226227, Done: False\n",
      "Step: 708, Reward: 0.1231033205986023, Done: False\n",
      "Step: 709, Reward: 0.11934086680412292, Done: False\n",
      "Step: 710, Reward: 0.11348120868206024, Done: False\n",
      "Step: 711, Reward: 0.1135106310248375, Done: False\n",
      "Step: 712, Reward: 0.112811379134655, Done: False\n",
      "Step: 713, Reward: 0.10906411707401276, Done: False\n",
      "Step: 714, Reward: 0.11081593483686447, Done: False\n",
      "Step: 715, Reward: 0.10770964622497559, Done: False\n",
      "Step: 716, Reward: 0.10644521564245224, Done: False\n",
      "Step: 717, Reward: 0.0974557101726532, Done: False\n",
      "Step: 718, Reward: 0.10022398084402084, Done: False\n",
      "Step: 719, Reward: 0.09379695355892181, Done: False\n",
      "Step: 720, Reward: 0.08934889733791351, Done: False\n",
      "Step: 721, Reward: 0.08845202624797821, Done: False\n",
      "Step: 722, Reward: 0.08747663348913193, Done: False\n",
      "Step: 723, Reward: 0.07892683893442154, Done: False\n",
      "Step: 724, Reward: 0.08004413545131683, Done: False\n",
      "Step: 725, Reward: 0.08041529357433319, Done: False\n",
      "Step: 726, Reward: 0.07802456617355347, Done: False\n",
      "Step: 727, Reward: 0.08183175325393677, Done: False\n",
      "Step: 728, Reward: 0.0833645835518837, Done: False\n",
      "Step: 729, Reward: 0.0823846384882927, Done: False\n",
      "Step: 730, Reward: 0.08101929724216461, Done: False\n",
      "Step: 731, Reward: 0.07645869255065918, Done: False\n",
      "Step: 732, Reward: 0.07353715598583221, Done: False\n",
      "Step: 733, Reward: 0.0757741630077362, Done: False\n",
      "Step: 734, Reward: 0.07564543187618256, Done: False\n",
      "Step: 735, Reward: 0.07490785419940948, Done: False\n",
      "Step: 736, Reward: 0.06839226186275482, Done: False\n",
      "Step: 737, Reward: 0.06742767244577408, Done: False\n",
      "Step: 738, Reward: 0.06768447160720825, Done: False\n",
      "Step: 739, Reward: 0.06163976714015007, Done: False\n",
      "Step: 740, Reward: 0.058130666613578796, Done: False\n",
      "Step: 741, Reward: 0.054429084062576294, Done: False\n",
      "Step: 742, Reward: 0.04999086260795593, Done: False\n",
      "Step: 743, Reward: 0.05149293690919876, Done: False\n",
      "Step: 744, Reward: 0.04736579954624176, Done: False\n",
      "Step: 745, Reward: 0.043496791273355484, Done: False\n",
      "Step: 746, Reward: 0.03858387470245361, Done: False\n",
      "Step: 747, Reward: 0.03386956453323364, Done: False\n",
      "Step: 748, Reward: 0.02976095676422119, Done: False\n",
      "Step: 749, Reward: 0.02961616963148117, Done: False\n",
      "Step: 750, Reward: 0.023285716772079468, Done: False\n",
      "Step: 751, Reward: 0.017178691923618317, Done: False\n",
      "Step: 752, Reward: 0.01244305819272995, Done: False\n",
      "Step: 753, Reward: 0.0037025362253189087, Done: False\n",
      "Step: 754, Reward: -0.007470905780792236, Done: False\n",
      "Step: 755, Reward: -0.020732365548610687, Done: False\n",
      "Step: 756, Reward: -0.03185223042964935, Done: False\n",
      "Step: 757, Reward: -0.037811748683452606, Done: False\n",
      "Step: 758, Reward: -0.04877695441246033, Done: False\n",
      "Step: 759, Reward: -0.06414914131164551, Done: False\n",
      "Step: 760, Reward: -0.08083690702915192, Done: False\n",
      "Step: 761, Reward: -0.097394660115242, Done: False\n",
      "Step: 762, Reward: -0.11833807080984116, Done: False\n",
      "Step: 763, Reward: -0.14526818692684174, Done: False\n",
      "Step: 764, Reward: -0.15933121740818024, Done: False\n",
      "Step: 765, Reward: -0.17169630527496338, Done: False\n",
      "Step: 766, Reward: -0.19067949056625366, Done: False\n",
      "Step: 767, Reward: -0.20935922861099243, Done: False\n",
      "Step: 768, Reward: -0.19472914934158325, Done: False\n",
      "Step: 769, Reward: -0.16857832670211792, Done: False\n",
      "Step: 770, Reward: -0.14179469645023346, Done: False\n",
      "Step: 771, Reward: -0.09321685880422592, Done: False\n",
      "Step: 772, Reward: -0.07772259414196014, Done: False\n",
      "Step: 773, Reward: -0.020665593445301056, Done: False\n",
      "Step: 774, Reward: -0.01929692178964615, Done: False\n",
      "Step: 775, Reward: -0.03268725425004959, Done: False\n",
      "Step: 776, Reward: -0.06391666829586029, Done: False\n",
      "Step: 777, Reward: -0.11554957181215286, Done: False\n",
      "Step: 778, Reward: -0.1507822722196579, Done: False\n",
      "Step: 779, Reward: -0.16921988129615784, Done: False\n",
      "Step: 780, Reward: -0.19668206572532654, Done: False\n",
      "Step: 781, Reward: -0.1964879333972931, Done: False\n",
      "Step: 782, Reward: -0.1696375012397766, Done: False\n",
      "Step: 783, Reward: -0.1434427797794342, Done: False\n",
      "Step: 784, Reward: -0.11325132101774216, Done: False\n",
      "Step: 785, Reward: -0.08312155306339264, Done: False\n",
      "Step: 786, Reward: -0.04596926271915436, Done: False\n",
      "Step: 787, Reward: -0.011152468621730804, Done: False\n",
      "Step: 788, Reward: -0.027590319514274597, Done: False\n",
      "Step: 789, Reward: -0.07686910033226013, Done: False\n",
      "Step: 790, Reward: -0.10656838119029999, Done: False\n",
      "Step: 791, Reward: -0.15526120364665985, Done: False\n",
      "Step: 792, Reward: -0.1847292184829712, Done: False\n",
      "Step: 793, Reward: -0.20615530014038086, Done: False\n",
      "Step: 794, Reward: -0.16546842455863953, Done: False\n",
      "Step: 795, Reward: -0.12780259549617767, Done: False\n",
      "Step: 796, Reward: -0.07686708122491837, Done: False\n",
      "Step: 797, Reward: -0.03336440026760101, Done: False\n",
      "Step: 798, Reward: -0.009802021086215973, Done: False\n",
      "Step: 799, Reward: -0.04445724934339523, Done: False\n",
      "Step: 800, Reward: -0.08929883688688278, Done: False\n",
      "Step: 801, Reward: -0.12660041451454163, Done: False\n",
      "Step: 802, Reward: -0.15591208636760712, Done: False\n",
      "Step: 803, Reward: -0.19561773538589478, Done: False\n",
      "Step: 804, Reward: -0.19569474458694458, Done: False\n",
      "Step: 805, Reward: -0.1739473044872284, Done: False\n",
      "Step: 806, Reward: -0.15335017442703247, Done: False\n",
      "Step: 807, Reward: -0.12659668922424316, Done: False\n",
      "Step: 808, Reward: -0.09167248010635376, Done: False\n",
      "Step: 809, Reward: -0.051802702248096466, Done: False\n",
      "Step: 810, Reward: 0.0028084442019462585, Done: False\n",
      "Step: 811, Reward: -0.03979241102933884, Done: False\n",
      "Step: 812, Reward: -0.07527953386306763, Done: False\n",
      "Step: 813, Reward: -0.10960519313812256, Done: False\n",
      "Step: 814, Reward: -0.15359748899936676, Done: False\n",
      "Step: 815, Reward: -0.16467279195785522, Done: False\n",
      "Step: 816, Reward: -0.1801539957523346, Done: False\n",
      "Step: 817, Reward: -0.18998205661773682, Done: False\n",
      "Step: 818, Reward: -0.2128564864397049, Done: False\n",
      "Step: 819, Reward: -0.20372365415096283, Done: False\n",
      "Step: 820, Reward: -0.18562912940979004, Done: False\n",
      "Step: 821, Reward: -0.17273329198360443, Done: False\n",
      "Step: 822, Reward: -0.15648886561393738, Done: False\n",
      "Step: 823, Reward: -0.13501524925231934, Done: False\n",
      "Step: 824, Reward: -0.12134506553411484, Done: False\n",
      "Step: 825, Reward: -0.05792917311191559, Done: False\n",
      "Step: 826, Reward: -0.01959778368473053, Done: False\n",
      "Step: 827, Reward: -0.03992346674203873, Done: False\n",
      "Step: 828, Reward: -0.0833001509308815, Done: False\n",
      "Step: 829, Reward: -0.1159130334854126, Done: False\n",
      "Step: 830, Reward: -0.1557752788066864, Done: False\n",
      "Step: 831, Reward: -0.17906862497329712, Done: False\n",
      "Step: 832, Reward: -0.2358798384666443, Done: False\n",
      "Step: 833, Reward: -0.20680847764015198, Done: False\n",
      "Step: 834, Reward: -0.17313772439956665, Done: False\n",
      "Step: 835, Reward: -0.14776983857154846, Done: False\n",
      "Step: 836, Reward: -0.1079854965209961, Done: False\n",
      "Step: 837, Reward: -0.05597202479839325, Done: False\n",
      "Step: 838, Reward: -0.03900201618671417, Done: False\n",
      "Step: 839, Reward: -0.0735258013010025, Done: False\n",
      "Step: 840, Reward: -0.0971447303891182, Done: False\n",
      "Step: 841, Reward: -0.15513479709625244, Done: False\n",
      "Step: 842, Reward: -0.17915287613868713, Done: False\n",
      "Step: 843, Reward: -0.2129686027765274, Done: False\n",
      "Step: 844, Reward: -0.23839572072029114, Done: False\n",
      "Step: 845, Reward: -0.21094992756843567, Done: False\n",
      "Step: 846, Reward: -0.17785680294036865, Done: False\n",
      "Step: 847, Reward: -0.1502266526222229, Done: False\n",
      "Step: 848, Reward: -0.12092162668704987, Done: False\n",
      "Step: 849, Reward: -0.07422184944152832, Done: False\n",
      "Step: 850, Reward: -0.04342963546514511, Done: False\n",
      "Step: 851, Reward: -0.009302385151386261, Done: False\n",
      "Step: 852, Reward: 0.0012577101588249207, Done: False\n",
      "Step: 853, Reward: -0.017628051340579987, Done: False\n",
      "Step: 854, Reward: -0.034799084067344666, Done: False\n",
      "Step: 855, Reward: -0.05246931314468384, Done: False\n",
      "Step: 856, Reward: -0.06311574578285217, Done: False\n",
      "Step: 857, Reward: -0.07340049743652344, Done: False\n",
      "Step: 858, Reward: -0.07676461338996887, Done: False\n",
      "Step: 859, Reward: -0.07982707023620605, Done: False\n",
      "Step: 860, Reward: -0.08480273187160492, Done: False\n",
      "Step: 861, Reward: -0.09116044640541077, Done: False\n",
      "Step: 862, Reward: -0.09795582294464111, Done: False\n",
      "Step: 863, Reward: -0.0627463161945343, Done: False\n",
      "Step: 864, Reward: -0.021059125661849976, Done: False\n",
      "Step: 865, Reward: 0.020859062671661377, Done: False\n",
      "Step: 866, Reward: 0.060666292905807495, Done: False\n",
      "Step: 867, Reward: 0.09862877428531647, Done: False\n",
      "Step: 868, Reward: 0.1371678113937378, Done: False\n",
      "Step: 869, Reward: 0.1801697015762329, Done: False\n",
      "Step: 870, Reward: 0.21459265053272247, Done: False\n",
      "Step: 871, Reward: 0.18713238835334778, Done: False\n",
      "Step: 872, Reward: 0.1560954749584198, Done: False\n",
      "Step: 873, Reward: 0.15480747818946838, Done: False\n",
      "Step: 874, Reward: 0.13841059803962708, Done: False\n",
      "Step: 875, Reward: 0.15259671211242676, Done: False\n",
      "Step: 876, Reward: 0.18488052487373352, Done: False\n",
      "Step: 877, Reward: 0.19683831930160522, Done: False\n",
      "Step: 878, Reward: 0.21833571791648865, Done: False\n",
      "Step: 879, Reward: 0.2416972517967224, Done: False\n",
      "Step: 880, Reward: 0.26795321702957153, Done: False\n",
      "Step: 881, Reward: 0.312115341424942, Done: False\n",
      "Step: 882, Reward: 0.36253324151039124, Done: False\n",
      "Step: 883, Reward: 0.419887900352478, Done: False\n",
      "Step: 884, Reward: 0.4885801672935486, Done: False\n",
      "Step: 885, Reward: 0.5586949586868286, Done: False\n",
      "Step: 886, Reward: 0.6417760848999023, Done: False\n",
      "Step: 887, Reward: 0.6445913910865784, Done: False\n",
      "Step: 888, Reward: 0.6258955001831055, Done: False\n",
      "Step: 889, Reward: 0.6331177949905396, Done: False\n",
      "Step: 890, Reward: 0.6453106999397278, Done: False\n",
      "Step: 891, Reward: 0.6457985043525696, Done: False\n",
      "Step: 892, Reward: 0.6566982865333557, Done: False\n",
      "Step: 893, Reward: 0.7190020084381104, Done: False\n",
      "Step: 894, Reward: 0.7770984172821045, Done: False\n",
      "Step: 895, Reward: 0.8307250142097473, Done: False\n",
      "Step: 896, Reward: 0.8682899475097656, Done: False\n",
      "Step: 897, Reward: 0.9122829437255859, Done: False\n",
      "Step: 898, Reward: 0.9654526710510254, Done: False\n",
      "Step: 899, Reward: 0.9770902395248413, Done: False\n",
      "Step 248: CRASHED on ground!\n",
      "Step: 900, Reward: -49.00455093383789, Done: True\n",
      "Drone starting at: (24.61, 39.93) m\n",
      "Environment Reset. Wind Dir: 282.2 deg. Moving Platform: True\n",
      "Step: 901, Reward: 0.15522879362106323, Done: False\n",
      "Step: 902, Reward: 0.15892121195793152, Done: False\n",
      "Step: 903, Reward: 0.15883882343769073, Done: False\n",
      "Step: 904, Reward: 0.1591273695230484, Done: False\n",
      "Step: 905, Reward: 0.15578600764274597, Done: False\n",
      "Step: 906, Reward: 0.15893889963626862, Done: False\n",
      "Step: 907, Reward: 0.15879499912261963, Done: False\n",
      "Step: 908, Reward: 0.15790782868862152, Done: False\n",
      "Step: 909, Reward: 0.15730592608451843, Done: False\n",
      "Step: 910, Reward: 0.15412667393684387, Done: False\n",
      "Step: 911, Reward: 0.1516970694065094, Done: False\n",
      "Step: 912, Reward: 0.1482653170824051, Done: False\n",
      "Step: 913, Reward: 0.14355263113975525, Done: False\n",
      "Step: 914, Reward: 0.13937968015670776, Done: False\n",
      "Step: 915, Reward: 0.13699987530708313, Done: False\n",
      "Step: 916, Reward: 0.13169342279434204, Done: False\n",
      "Step: 917, Reward: 0.128779336810112, Done: False\n",
      "Step: 918, Reward: 0.12582296133041382, Done: False\n",
      "Step: 919, Reward: 0.12671537697315216, Done: False\n",
      "Step: 920, Reward: 0.12772232294082642, Done: False\n",
      "Step: 921, Reward: 0.12955747544765472, Done: False\n",
      "Step: 922, Reward: 0.12446770817041397, Done: False\n",
      "Step: 923, Reward: 0.12899737060070038, Done: False\n",
      "Step: 924, Reward: 0.12896521389484406, Done: False\n",
      "Step: 925, Reward: 0.1255570501089096, Done: False\n",
      "Step: 926, Reward: 0.12848173081874847, Done: False\n",
      "Step: 927, Reward: 0.12837423384189606, Done: False\n",
      "Step: 928, Reward: 0.12483373284339905, Done: False\n",
      "Step: 929, Reward: 0.12425823509693146, Done: False\n",
      "Step: 930, Reward: 0.12084788084030151, Done: False\n",
      "Step: 931, Reward: 0.1155007854104042, Done: False\n",
      "Step: 932, Reward: 0.11162007600069046, Done: False\n",
      "Step: 933, Reward: 0.11034268140792847, Done: False\n",
      "Step: 934, Reward: 0.10850512981414795, Done: False\n",
      "Step: 935, Reward: 0.11237078905105591, Done: False\n",
      "Step: 936, Reward: 0.11663645505905151, Done: False\n",
      "Step: 937, Reward: 0.12534096837043762, Done: False\n",
      "Step: 938, Reward: 0.1248035728931427, Done: False\n",
      "Step: 939, Reward: 0.12622039020061493, Done: False\n",
      "Step: 940, Reward: 0.12460287660360336, Done: False\n",
      "Step: 941, Reward: 0.1252419650554657, Done: False\n",
      "Step: 942, Reward: 0.12406191974878311, Done: False\n",
      "Step: 943, Reward: 0.12612903118133545, Done: False\n",
      "Step: 944, Reward: 0.12319640815258026, Done: False\n",
      "Step: 945, Reward: 0.12149849534034729, Done: False\n",
      "Step: 946, Reward: 0.11729012429714203, Done: False\n",
      "Step: 947, Reward: 0.11836922913789749, Done: False\n",
      "Step: 948, Reward: 0.11603779345750809, Done: False\n",
      "Step: 949, Reward: 0.11444047838449478, Done: False\n",
      "Step: 950, Reward: 0.112371064722538, Done: False\n",
      "Step: 951, Reward: 0.10780510306358337, Done: False\n",
      "Step: 952, Reward: 0.10463260859251022, Done: False\n",
      "Step: 953, Reward: 0.10311342030763626, Done: False\n",
      "Step: 954, Reward: 0.10577531903982162, Done: False\n",
      "Step: 955, Reward: 0.10486041009426117, Done: False\n",
      "Step: 956, Reward: 0.10548616200685501, Done: False\n",
      "Step: 957, Reward: 0.1039758250117302, Done: False\n",
      "Step: 958, Reward: 0.1022382602095604, Done: False\n",
      "Step: 959, Reward: 0.0978035256266594, Done: False\n",
      "Step: 960, Reward: 0.09102048724889755, Done: False\n",
      "Step: 961, Reward: 0.08759599924087524, Done: False\n",
      "Step: 962, Reward: 0.08868265151977539, Done: False\n",
      "Step: 963, Reward: 0.08636054396629333, Done: False\n",
      "Step: 964, Reward: 0.08313769102096558, Done: False\n",
      "Step: 965, Reward: 0.08203615248203278, Done: False\n",
      "Step: 966, Reward: 0.08000752329826355, Done: False\n",
      "Step: 967, Reward: 0.07702910900115967, Done: False\n",
      "Step: 968, Reward: 0.07580970227718353, Done: False\n",
      "Step: 969, Reward: 0.07273352891206741, Done: False\n",
      "Step: 970, Reward: 0.0701168030500412, Done: False\n",
      "Step: 971, Reward: 0.06985940039157867, Done: False\n",
      "Step: 972, Reward: 0.06904888153076172, Done: False\n",
      "Step: 973, Reward: 0.06383955478668213, Done: False\n",
      "Step: 974, Reward: 0.05995165556669235, Done: False\n",
      "Step: 975, Reward: 0.05034133791923523, Done: False\n",
      "Step: 976, Reward: 0.05268685892224312, Done: False\n",
      "Step: 977, Reward: 0.04675133153796196, Done: False\n",
      "Step: 978, Reward: 0.04589476436376572, Done: False\n",
      "Step: 979, Reward: 0.036670856177806854, Done: False\n",
      "Step: 980, Reward: 0.035094521939754486, Done: False\n",
      "Step: 981, Reward: 0.03536572307348251, Done: False\n",
      "Step: 982, Reward: 0.03748863562941551, Done: False\n",
      "Step: 983, Reward: 0.037014007568359375, Done: False\n",
      "Step: 984, Reward: 0.03459761291742325, Done: False\n",
      "Step: 985, Reward: 0.029433855786919594, Done: False\n",
      "Step: 986, Reward: 0.02782141976058483, Done: False\n",
      "Step: 987, Reward: 0.021340642124414444, Done: False\n",
      "Step: 988, Reward: 0.016807392239570618, Done: False\n",
      "Step: 989, Reward: 0.013220863416790962, Done: False\n",
      "Step: 990, Reward: 0.01284797489643097, Done: False\n",
      "Step: 991, Reward: 0.015248667448759079, Done: False\n",
      "Step: 992, Reward: 0.017820337787270546, Done: False\n",
      "Step: 993, Reward: 0.014503860846161842, Done: False\n",
      "Step: 994, Reward: 0.015806788578629494, Done: False\n",
      "Step: 995, Reward: 0.01459595374763012, Done: False\n",
      "Step: 996, Reward: 0.015064911916851997, Done: False\n",
      "Step: 997, Reward: 0.015979625284671783, Done: False\n",
      "Step: 998, Reward: 0.011838745325803757, Done: False\n",
      "Step: 999, Reward: 0.008705860003829002, Done: False\n",
      "Step: 1000, Reward: 0.007430866360664368, Done: False\n",
      "Step: 1001, Reward: 0.00912974588572979, Done: False\n",
      "Step: 1002, Reward: 0.00877823680639267, Done: False\n",
      "Step: 1003, Reward: 0.005494080483913422, Done: False\n",
      "Step: 1004, Reward: 0.0020554792135953903, Done: False\n",
      "Step: 1005, Reward: 0.003257583826780319, Done: False\n",
      "Step: 1006, Reward: 0.003147270530462265, Done: False\n",
      "Step: 1007, Reward: 0.0034722210839390755, Done: False\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gym\n",
    "# Make sure you can import your custom environment\n",
    "# If installed as a package, you might do:\n",
    "# from drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "# Otherwise, adjust your import path accordingly:\n",
    "from drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "\n",
    "def main():\n",
    "    # Example: Create an environment with a moving platform\n",
    "    env = Drone2dEnv(render_sim=True, moving_platform=True, platform_speed=8.0)\n",
    "\n",
    "    # Example: Create an environment with a static platform (default)\n",
    "    #env = Drone2dEnv(render_sim=True)\n",
    "    # or explicitly\n",
    "    #env = Drone2dEnv(render_sim=True, moving_platform=False)\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for step in range(3000):\n",
    "        # Sample a random action from the action space\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Step the environment\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        \n",
    "        # Print some debug info if desired\n",
    "        print(f\"Step: {step}, Reward: {reward}, Done: {done}\")\n",
    "        \n",
    "        if done:\n",
    "            # If the episode ended (e.g., drone crashed or time ran out),\n",
    "            # reset for another episode\n",
    "            obs = env.reset()\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Size: (11,)\n",
      "Drone starting at: (25.06, 37.74) m\n",
      "Environment Reset. Wind Dir: 162.1 deg. Moving Platform: False\n",
      "Action Space: Box(-1.0, 1.0, (2,), float32)\n",
      "Observation Space: Box([-1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  0.], 1.0, (11,), float32)\n",
      "Observation Space Sample: [-0.3389212  -0.13802144 -0.8785918  -0.7325835   0.37101918 -0.03264457\n",
      "  0.3932357  -0.4477329  -0.48124826  0.5280521   0.7294194 ]\n",
      "Drone starting at: (20.15, 48.86) m\n",
      "Environment Reset. Wind Dir: 138.5 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 1 Starting ---\n",
      "Step 30: OUT OF BOUNDS!\n",
      "--- Episode 1 Finished ---\n",
      "Steps: 31, Total Reward: -45.93\n",
      "Final Info: {'Battery': np.float32(98.98598), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 31, 'raw_pos': (19.276013124735567, 50.01918026482656), 'raw_vel': (-3.294981769403548, 4.83239688902021), 'raw_angle_rad': 0.14024114748283104, 'raw_angular_vel': -0.6381558924423315, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (19.86, 45.82) m\n",
      "Environment Reset. Wind Dir: 237.7 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 2 Starting ---\n",
      "Step 91: OUT OF BOUNDS!\n",
      "--- Episode 2 Finished ---\n",
      "Steps: 92, Total Reward: -38.08\n",
      "Final Info: {'Battery': np.float32(97.326675), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 92, 'raw_pos': (16.112548425989694, 50.05271587496669), 'raw_vel': (-4.18575297788707, 4.527674426814343), 'raw_angle_rad': -0.20602703956565294, 'raw_angular_vel': -0.4294476712157819, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (27.41, 32.15) m\n",
      "Environment Reset. Wind Dir: 79.2 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 3 Starting ---\n",
      "Step 112: OUT OF BOUNDS!\n",
      "--- Episode 3 Finished ---\n",
      "Steps: 113, Total Reward: -36.08\n",
      "Final Info: {'Battery': np.float32(96.7235), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 113, 'raw_pos': (30.112497772199415, 50.04069270299531), 'raw_vel': (5.357038781549768, 15.160027689304094), 'raw_angle_rad': -0.7649157126920375, 'raw_angular_vel': -1.130782132318761, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (18.72, 45.58) m\n",
      "Environment Reset. Wind Dir: 315.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 4 Starting ---\n",
      "Step 81: OUT OF BOUNDS!\n",
      "--- Episode 4 Finished ---\n",
      "Steps: 82, Total Reward: -38.57\n",
      "Final Info: {'Battery': np.float32(97.591484), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 82, 'raw_pos': (21.054438765193623, 50.062518147818025), 'raw_vel': (2.9942120653421505, 5.041117923969538), 'raw_angle_rad': 0.15826153170959334, 'raw_angular_vel': 1.0699546310093946, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (31.40, 40.49) m\n",
      "Environment Reset. Wind Dir: 78.0 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 5 Starting ---\n",
      "Step 78: OUT OF BOUNDS!\n",
      "--- Episode 5 Finished ---\n",
      "Steps: 79, Total Reward: -41.25\n",
      "Final Info: {'Battery': np.float32(97.69899), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 79, 'raw_pos': (32.65550720384793, 50.19445133542884), 'raw_vel': (0.9021246651816102, 11.135598678539505), 'raw_angle_rad': 0.40856361808063124, 'raw_angular_vel': 0.6831273496045283, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (29.36, 48.68) m\n",
      "Environment Reset. Wind Dir: 50.8 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 6 Starting ---\n",
      "Step 30: OUT OF BOUNDS!\n",
      "--- Episode 6 Finished ---\n",
      "Steps: 31, Total Reward: -45.43\n",
      "Final Info: {'Battery': np.float32(98.99893), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 31, 'raw_pos': (29.718481570111916, 50.086153845564446), 'raw_vel': (1.0273331327732809, 5.124009591957892), 'raw_angle_rad': -0.026157044406630123, 'raw_angular_vel': -0.28720967262929703, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (27.61, 47.30) m\n",
      "Environment Reset. Wind Dir: 120.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 7 Starting ---\n",
      "Step 49: OUT OF BOUNDS!\n",
      "--- Episode 7 Finished ---\n",
      "Steps: 50, Total Reward: -44.87\n",
      "Final Info: {'Battery': np.float32(98.48105), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 50, 'raw_pos': (28.102429510543583, 50.11506059003419), 'raw_vel': (-1.282315215357274, 5.973678502375134), 'raw_angle_rad': 1.5402854931772822, 'raw_angular_vel': 11.405447833285278, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (33.99, 38.15) m\n",
      "Environment Reset. Wind Dir: 116.4 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 8 Starting ---\n",
      "Step 92: OUT OF BOUNDS!\n",
      "--- Episode 8 Finished ---\n",
      "Steps: 93, Total Reward: -41.00\n",
      "Final Info: {'Battery': np.float32(97.13082), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 93, 'raw_pos': (28.725901503941078, 50.10800392319267), 'raw_vel': (-7.430734169242329, 13.85522292299515), 'raw_angle_rad': 0.30785226492562195, 'raw_angular_vel': -0.5040890168527741, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (30.18, 40.85) m\n",
      "Environment Reset. Wind Dir: 97.9 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 9 Starting ---\n",
      "Step 85: OUT OF BOUNDS!\n",
      "--- Episode 9 Finished ---\n",
      "Steps: 86, Total Reward: -39.55\n",
      "Final Info: {'Battery': np.float32(97.61159), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 86, 'raw_pos': (28.37920039487233, 50.06274823687799), 'raw_vel': (-2.956100744916712, 10.935889150586084), 'raw_angle_rad': 0.17186797103382934, 'raw_angular_vel': 0.910681247775663, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (29.89, 47.95) m\n",
      "Environment Reset. Wind Dir: 355.8 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 10 Starting ---\n",
      "Step 42: OUT OF BOUNDS!\n",
      "--- Episode 10 Finished ---\n",
      "Steps: 43, Total Reward: -44.23\n",
      "Final Info: {'Battery': np.float32(98.64468), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 43, 'raw_pos': (31.277325305276797, 50.04201755487961), 'raw_vel': (3.0842483342370155, 4.840082731427347), 'raw_angle_rad': 0.15993634026861786, 'raw_angular_vel': 0.470318036711486, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (20.28, 42.34) m\n",
      "Environment Reset. Wind Dir: 251.0 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 11 Starting ---\n",
      "Step 94: OUT OF BOUNDS!\n",
      "--- Episode 11 Finished ---\n",
      "Steps: 95, Total Reward: -37.41\n",
      "Final Info: {'Battery': np.float32(96.914764), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 95, 'raw_pos': (19.577623257023, 50.06748137367939), 'raw_vel': (-1.2909450414096706, 7.496238861879298), 'raw_angle_rad': 0.33937497356752616, 'raw_angular_vel': 1.1324094254701014, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (29.68, 40.51) m\n",
      "Environment Reset. Wind Dir: 128.8 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 12 Starting ---\n",
      "Step 73: OUT OF BOUNDS!\n",
      "--- Episode 12 Finished ---\n",
      "Steps: 74, Total Reward: -41.43\n",
      "Final Info: {'Battery': np.float32(97.60842), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 74, 'raw_pos': (28.411477570607158, 50.052018158372064), 'raw_vel': (-0.6581546390283463, 11.937379118166328), 'raw_angle_rad': -0.6087012498218698, 'raw_angular_vel': -1.4183928014920126, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (24.61, 37.90) m\n",
      "Environment Reset. Wind Dir: 222.8 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 13 Starting ---\n",
      "Step 125: LOST CONTROL!\n",
      "--- Episode 13 Finished ---\n",
      "Steps: 126, Total Reward: -34.72\n",
      "Final Info: {'Battery': np.float32(96.291664), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 126, 'raw_pos': (17.746098500967815, 47.78820621073394), 'raw_vel': (-5.50242118344521, 7.014369238433113), 'raw_angle_rad': -1.5780495829597134, 'raw_angular_vel': -14.136386682238426, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (20.18, 40.97) m\n",
      "Environment Reset. Wind Dir: 83.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 14 Starting ---\n",
      "Step 76: OUT OF BOUNDS!\n",
      "--- Episode 14 Finished ---\n",
      "Steps: 77, Total Reward: -41.51\n",
      "Final Info: {'Battery': np.float32(97.65364), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 77, 'raw_pos': (18.024561611095724, 50.07012888092905), 'raw_vel': (-2.926384177103797, 11.555619969373845), 'raw_angle_rad': -0.20662664401667302, 'raw_angular_vel': -1.4612909399675122, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (19.67, 48.41) m\n",
      "Environment Reset. Wind Dir: 76.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 15 Starting ---\n",
      "Step 30: OUT OF BOUNDS!\n",
      "--- Episode 15 Finished ---\n",
      "Steps: 31, Total Reward: -45.87\n",
      "Final Info: {'Battery': np.float32(99.04591), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 31, 'raw_pos': (19.925684132072917, 50.02143935255155), 'raw_vel': (1.5946477502898226, 4.787316623731998), 'raw_angle_rad': -0.2937704308516886, 'raw_angular_vel': 0.9625962733523346, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (33.38, 33.30) m\n",
      "Environment Reset. Wind Dir: 6.7 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 16 Starting ---\n",
      "Step 117: OUT OF BOUNDS!\n",
      "--- Episode 16 Finished ---\n",
      "Steps: 118, Total Reward: -45.38\n",
      "Final Info: {'Battery': np.float32(96.17835), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 118, 'raw_pos': (50.16034882316842, 49.606687089642634), 'raw_vel': (15.43752813855726, 14.266421979210119), 'raw_angle_rad': -0.5828435451296367, 'raw_angular_vel': 0.15298491532033998, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (21.57, 48.93) m\n",
      "Environment Reset. Wind Dir: 330.9 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 17 Starting ---\n",
      "Step 48: OUT OF BOUNDS!\n",
      "--- Episode 17 Finished ---\n",
      "Steps: 49, Total Reward: -43.42\n",
      "Final Info: {'Battery': np.float32(98.58425), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 49, 'raw_pos': (21.030788755507736, 50.027242121524104), 'raw_vel': (-1.3851604664027117, 2.7759870349352553), 'raw_angle_rad': 0.21135492097441463, 'raw_angular_vel': 1.0035519246119073, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (29.38, 31.89) m\n",
      "Environment Reset. Wind Dir: 208.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 18 Starting ---\n",
      "Step 117: LOST CONTROL!\n",
      "--- Episode 18 Finished ---\n",
      "Steps: 118, Total Reward: -31.69\n",
      "Final Info: {'Battery': np.float32(96.74065), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 118, 'raw_pos': (20.989323779926817, 37.872847208120454), 'raw_vel': (-5.433026260470805, 4.530402052754871), 'raw_angle_rad': -1.6211918306835393, 'raw_angular_vel': -9.789632332224144, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (22.08, 47.17) m\n",
      "Environment Reset. Wind Dir: 342.6 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 19 Starting ---\n",
      "Step 60: OUT OF BOUNDS!\n",
      "--- Episode 19 Finished ---\n",
      "Steps: 61, Total Reward: -41.95\n",
      "Final Info: {'Battery': np.float32(98.23605), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 61, 'raw_pos': (23.07469588962737, 50.00060906015347), 'raw_vel': (2.1630317978340945, 4.453516829324488), 'raw_angle_rad': 0.5569172892616239, 'raw_angular_vel': 4.53398139325854, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (18.61, 31.14) m\n",
      "Environment Reset. Wind Dir: 306.5 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 20 Starting ---\n",
      "Step 139: LOST CONTROL!\n",
      "--- Episode 20 Finished ---\n",
      "Steps: 140, Total Reward: -29.67\n",
      "Final Info: {'Battery': np.float32(95.91831), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 140, 'raw_pos': (15.748191645459892, 41.437351600056274), 'raw_vel': (-5.14939208923077, 5.4163582388091385), 'raw_angle_rad': 1.5722320616667647, 'raw_angular_vel': 8.981253881844918, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (17.43, 44.17) m\n",
      "Environment Reset. Wind Dir: 64.7 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 21 Starting ---\n",
      "Step 55: LOST CONTROL!\n",
      "--- Episode 21 Finished ---\n",
      "Steps: 56, Total Reward: -45.38\n",
      "Final Info: {'Battery': np.float32(98.26642), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 56, 'raw_pos': (17.804738622961068, 48.33719305671415), 'raw_vel': (4.741362726210008, 6.075263035176891), 'raw_angle_rad': -1.6779006150882227, 'raw_angular_vel': -11.120901470929022, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (21.18, 31.22) m\n",
      "Environment Reset. Wind Dir: 341.0 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 22 Starting ---\n",
      "Step 130: LOST CONTROL!\n",
      "--- Episode 22 Finished ---\n",
      "Steps: 131, Total Reward: -31.82\n",
      "Final Info: {'Battery': np.float32(95.89262), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 131, 'raw_pos': (21.964641564402978, 47.13750353292636), 'raw_vel': (-1.6435446583576054, 11.205851089875452), 'raw_angle_rad': 1.6002947839498238, 'raw_angular_vel': 9.208614248778883, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (31.33, 38.96) m\n",
      "Environment Reset. Wind Dir: 151.4 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 23 Starting ---\n",
      "Step 87: LOST CONTROL!\n",
      "--- Episode 23 Finished ---\n",
      "Steps: 88, Total Reward: -41.60\n",
      "Final Info: {'Battery': np.float32(97.42471), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 88, 'raw_pos': (23.427930846704538, 47.089496615796115), 'raw_vel': (-11.424277121257935, 8.133370207574204), 'raw_angle_rad': 1.6593207478177774, 'raw_angular_vel': 7.5445517094971635, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (16.46, 48.37) m\n",
      "Environment Reset. Wind Dir: 314.5 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 24 Starting ---\n",
      "Step 43: OUT OF BOUNDS!\n",
      "--- Episode 24 Finished ---\n",
      "Steps: 44, Total Reward: -44.58\n",
      "Final Info: {'Battery': np.float32(98.63888), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 44, 'raw_pos': (16.24624700669285, 50.00103291334218), 'raw_vel': (-0.3318912208386785, 3.2026459221256274), 'raw_angle_rad': -0.11414451248785067, 'raw_angular_vel': -0.24030232500676482, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (26.48, 45.70) m\n",
      "Environment Reset. Wind Dir: 301.5 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 25 Starting ---\n",
      "Step 94: OUT OF BOUNDS!\n",
      "--- Episode 25 Finished ---\n",
      "Steps: 95, Total Reward: -36.83\n",
      "Final Info: {'Battery': np.float32(97.19528), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 95, 'raw_pos': (27.49404056501797, 50.064798427197466), 'raw_vel': (-1.511066198517095, 4.124684324726205), 'raw_angle_rad': 0.9553713819829658, 'raw_angular_vel': 1.8506247450683906, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (24.37, 31.31) m\n",
      "Environment Reset. Wind Dir: 261.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 26 Starting ---\n",
      "Step 103: LOST CONTROL!\n",
      "--- Episode 26 Finished ---\n",
      "Steps: 104, Total Reward: -33.85\n",
      "Final Info: {'Battery': np.float32(96.634125), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 104, 'raw_pos': (24.159932283298513, 40.11671334809582), 'raw_vel': (-1.1394296476106545, 6.410985718703394), 'raw_angle_rad': -1.5981015420639888, 'raw_angular_vel': -13.48535129643841, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (27.69, 40.23) m\n",
      "Environment Reset. Wind Dir: 148.0 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 27 Starting ---\n",
      "Step 67: LOST CONTROL!\n",
      "--- Episode 27 Finished ---\n",
      "Steps: 68, Total Reward: -42.71\n",
      "Final Info: {'Battery': np.float32(97.8833), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 68, 'raw_pos': (27.950492930136264, 46.669618241444496), 'raw_vel': (1.2450150652217267, 8.287570396638554), 'raw_angle_rad': -1.633095312837242, 'raw_angular_vel': -11.683680091082135, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (26.44, 38.21) m\n",
      "Environment Reset. Wind Dir: 82.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 28 Starting ---\n",
      "Step 74: LOST CONTROL!\n",
      "--- Episode 28 Finished ---\n",
      "Steps: 75, Total Reward: -43.25\n",
      "Final Info: {'Battery': np.float32(97.51613), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 75, 'raw_pos': (29.368668013840537, 47.96502657231413), 'raw_vel': (-0.5102764407462208, 11.123245651828537), 'raw_angle_rad': 1.7475259728562267, 'raw_angular_vel': 14.368249548349882, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (29.28, 35.34) m\n",
      "Environment Reset. Wind Dir: 94.2 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 29 Starting ---\n",
      "Step 99: OUT OF BOUNDS!\n",
      "--- Episode 29 Finished ---\n",
      "Steps: 100, Total Reward: -41.32\n",
      "Final Info: {'Battery': np.float32(96.87143), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 100, 'raw_pos': (32.12799644643488, 50.027348985055205), 'raw_vel': (2.245735635148294, 14.96616189019537), 'raw_angle_rad': -1.393134660212868, 'raw_angular_vel': -10.952203086789737, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (28.63, 47.75) m\n",
      "Environment Reset. Wind Dir: 289.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 30 Starting ---\n",
      "Step 52: LOST CONTROL!\n",
      "--- Episode 30 Finished ---\n",
      "Steps: 53, Total Reward: -44.65\n",
      "Final Info: {'Battery': np.float32(98.542336), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 53, 'raw_pos': (30.67989647284346, 48.20890559618903), 'raw_vel': (1.1354015013812315, -0.05808512662202081), 'raw_angle_rad': 1.7729519244923195, 'raw_angular_vel': 13.70401633400927, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (16.39, 46.95) m\n",
      "Environment Reset. Wind Dir: 143.9 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 31 Starting ---\n",
      "Step 50: OUT OF BOUNDS!\n",
      "--- Episode 31 Finished ---\n",
      "Steps: 51, Total Reward: -45.04\n",
      "Final Info: {'Battery': np.float32(98.49084), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 51, 'raw_pos': (14.477036195180316, 50.07703996883093), 'raw_vel': (-1.5992559285396524, 5.655542985429578), 'raw_angle_rad': -0.7013076595616077, 'raw_angular_vel': 0.6120669691689005, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (33.34, 38.39) m\n",
      "Environment Reset. Wind Dir: 302.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 32 Starting ---\n",
      "Step 136: LOST CONTROL!\n",
      "--- Episode 32 Finished ---\n",
      "Steps: 137, Total Reward: -42.61\n",
      "Final Info: {'Battery': np.float32(95.836945), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 137, 'raw_pos': (48.929124177044514, 48.66376586147719), 'raw_vel': (12.029153446558954, 6.071636402255576), 'raw_angle_rad': -1.7835060896235018, 'raw_angular_vel': -14.081554518337466, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (18.15, 47.53) m\n",
      "Environment Reset. Wind Dir: 227.7 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 33 Starting ---\n",
      "Step 56: OUT OF BOUNDS!\n",
      "--- Episode 33 Finished ---\n",
      "Steps: 57, Total Reward: -42.21\n",
      "Final Info: {'Battery': np.float32(98.26042), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 57, 'raw_pos': (18.09907741692117, 50.06372963199269), 'raw_vel': (-0.24126105955139102, 4.018173667421045), 'raw_angle_rad': -0.0207858086878668, 'raw_angular_vel': 1.2153875651791843, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (29.19, 44.77) m\n",
      "Environment Reset. Wind Dir: 340.8 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 34 Starting ---\n",
      "Step 87: OUT OF BOUNDS!\n",
      "--- Episode 34 Finished ---\n",
      "Steps: 88, Total Reward: -37.60\n",
      "Final Info: {'Battery': np.float32(97.374725), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 88, 'raw_pos': (31.454654278720895, 50.05137970592948), 'raw_vel': (2.324493496178905, 7.429801144689341), 'raw_angle_rad': 0.13004719876558896, 'raw_angular_vel': -0.17219726548118477, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (22.03, 47.16) m\n",
      "Environment Reset. Wind Dir: 51.4 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 35 Starting ---\n",
      "Step 45: OUT OF BOUNDS!\n",
      "--- Episode 35 Finished ---\n",
      "Steps: 46, Total Reward: -43.46\n",
      "Final Info: {'Battery': np.float32(98.66254), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 46, 'raw_pos': (22.003133600011886, 50.00735450939272), 'raw_vel': (-0.12278704692514758, 6.034987505950367), 'raw_angle_rad': 0.27525140312735996, 'raw_angular_vel': 1.096617540936697, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (31.78, 44.77) m\n",
      "Environment Reset. Wind Dir: 49.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 36 Starting ---\n",
      "Step 67: OUT OF BOUNDS!\n",
      "--- Episode 36 Finished ---\n",
      "Steps: 68, Total Reward: -41.43\n",
      "Final Info: {'Battery': np.float32(98.072754), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 68, 'raw_pos': (33.44730565589145, 50.13999775597911), 'raw_vel': (2.9592541531840064, 8.418205555282478), 'raw_angle_rad': 0.010445591475403086, 'raw_angular_vel': 1.4643512349740722, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (20.70, 38.01) m\n",
      "Environment Reset. Wind Dir: 224.0 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 37 Starting ---\n",
      "Step 128: LOST CONTROL!\n",
      "--- Episode 37 Finished ---\n",
      "Steps: 129, Total Reward: -38.52\n",
      "Final Info: {'Battery': np.float32(96.062065), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 129, 'raw_pos': (8.579287207397185, 49.604982393663455), 'raw_vel': (-10.769915874784571, 8.045698687548372), 'raw_angle_rad': 1.58316514614163, 'raw_angular_vel': 9.834840456296806, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (22.09, 40.41) m\n",
      "Environment Reset. Wind Dir: 145.9 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 38 Starting ---\n",
      "Step 96: OUT OF BOUNDS!\n",
      "--- Episode 38 Finished ---\n",
      "Steps: 97, Total Reward: -39.10\n",
      "Final Info: {'Battery': np.float32(97.31773), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 97, 'raw_pos': (23.59205064424994, 50.08369404365818), 'raw_vel': (3.6741381519047036, 8.27402980818356), 'raw_angle_rad': -1.2126355059254277, 'raw_angular_vel': -3.003876043857768, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (29.51, 38.25) m\n",
      "Environment Reset. Wind Dir: 252.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 39 Starting ---\n",
      "Step 124: LOST CONTROL!\n",
      "--- Episode 39 Finished ---\n",
      "Steps: 125, Total Reward: -34.81\n",
      "Final Info: {'Battery': np.float32(96.345604), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 125, 'raw_pos': (26.950746506112207, 44.87139232578916), 'raw_vel': (-7.20840936505091, 2.95080423662165), 'raw_angle_rad': 1.7129180226044391, 'raw_angular_vel': 9.54351384154225, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (24.18, 43.35) m\n",
      "Environment Reset. Wind Dir: 354.7 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 40 Starting ---\n",
      "Step 89: OUT OF BOUNDS!\n",
      "--- Episode 40 Finished ---\n",
      "Steps: 90, Total Reward: -39.15\n",
      "Final Info: {'Battery': np.float32(97.2756), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 90, 'raw_pos': (32.47622917998275, 50.02004360748556), 'raw_vel': (10.403723081614446, 8.433723343381665), 'raw_angle_rad': -0.1849043750486154, 'raw_angular_vel': -0.546047503734031, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (24.98, 38.21) m\n",
      "Environment Reset. Wind Dir: 274.2 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 41 Starting ---\n",
      "Step 125: LOST CONTROL!\n",
      "--- Episode 41 Finished ---\n",
      "Steps: 126, Total Reward: -33.50\n",
      "Final Info: {'Battery': np.float32(96.1985), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 126, 'raw_pos': (27.572334816280833, 46.10982761923231), 'raw_vel': (6.077967033112982, 4.884602999587367), 'raw_angle_rad': -1.6704839642535672, 'raw_angular_vel': -9.588130429791297, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (26.68, 41.43) m\n",
      "Environment Reset. Wind Dir: 252.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 42 Starting ---\n",
      "Step 126: LOST CONTROL!\n",
      "--- Episode 42 Finished ---\n",
      "Steps: 127, Total Reward: -33.49\n",
      "Final Info: {'Battery': np.float32(96.28471), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 127, 'raw_pos': (21.44444720243679, 49.04936060683316), 'raw_vel': (-1.2682766351750092, 4.644803874733219), 'raw_angle_rad': -1.634881954890949, 'raw_angular_vel': -12.89701291995086, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (28.87, 36.74) m\n",
      "Environment Reset. Wind Dir: 82.8 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 43 Starting ---\n",
      "Step 75: LOST CONTROL!\n",
      "--- Episode 43 Finished ---\n",
      "Steps: 76, Total Reward: -42.64\n",
      "Final Info: {'Battery': np.float32(97.60876), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 76, 'raw_pos': (25.14205467628025, 45.25292051485239), 'raw_vel': (-5.871795840913204, 10.935344253323926), 'raw_angle_rad': 1.6322655330142837, 'raw_angular_vel': 9.001324463296998, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (27.15, 42.01) m\n",
      "Environment Reset. Wind Dir: 127.7 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 44 Starting ---\n",
      "Step 79: OUT OF BOUNDS!\n",
      "--- Episode 44 Finished ---\n",
      "Steps: 80, Total Reward: -40.11\n",
      "Final Info: {'Battery': np.float32(97.716), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 80, 'raw_pos': (22.04688058978403, 50.040277122290355), 'raw_vel': (-6.769038273447322, 9.842589620293042), 'raw_angle_rad': 0.3613577514814757, 'raw_angular_vel': -0.2460853076345525, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (30.01, 44.40) m\n",
      "Environment Reset. Wind Dir: 91.9 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 45 Starting ---\n",
      "Step 63: OUT OF BOUNDS!\n",
      "--- Episode 45 Finished ---\n",
      "Steps: 64, Total Reward: -42.80\n",
      "Final Info: {'Battery': np.float32(98.10439), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 64, 'raw_pos': (26.54338750077913, 50.06484617497478), 'raw_vel': (-6.036011832339695, 8.392811533698039), 'raw_angle_rad': 0.03433585919494167, 'raw_angular_vel': -1.2872908406141517, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (23.26, 48.73) m\n",
      "Environment Reset. Wind Dir: 26.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 46 Starting ---\n",
      "Step 40: OUT OF BOUNDS!\n",
      "--- Episode 46 Finished ---\n",
      "Steps: 41, Total Reward: -43.76\n",
      "Final Info: {'Battery': np.float32(98.957664), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 41, 'raw_pos': (23.832626356852025, 50.02829043240694), 'raw_vel': (1.265057039928939, 3.2601540510105096), 'raw_angle_rad': 0.16530919217885315, 'raw_angular_vel': 0.32813084660470343, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (17.66, 36.91) m\n",
      "Environment Reset. Wind Dir: 82.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 47 Starting ---\n",
      "Step 94: OUT OF BOUNDS!\n",
      "--- Episode 47 Finished ---\n",
      "Steps: 95, Total Reward: -41.03\n",
      "Final Info: {'Battery': np.float32(97.175804), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 95, 'raw_pos': (19.851731592516416, 50.20340972380614), 'raw_vel': (0.6615374563588523, 13.428316843891325), 'raw_angle_rad': -0.5778446767378527, 'raw_angular_vel': -5.768759435359881, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (24.37, 37.29) m\n",
      "Environment Reset. Wind Dir: 150.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 48 Starting ---\n",
      "Step 111: OUT OF BOUNDS!\n",
      "--- Episode 48 Finished ---\n",
      "Steps: 112, Total Reward: -40.23\n",
      "Final Info: {'Battery': np.float32(96.71591), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 112, 'raw_pos': (10.746083159469565, 50.15697326449589), 'raw_vel': (-13.113163476496652, 12.494854202425708), 'raw_angle_rad': 0.3324086104294057, 'raw_angular_vel': 0.457877932836387, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (33.39, 34.39) m\n",
      "Environment Reset. Wind Dir: 327.0 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 49 Starting ---\n",
      "Step 110: LOST CONTROL!\n",
      "--- Episode 49 Finished ---\n",
      "Steps: 111, Total Reward: -40.68\n",
      "Final Info: {'Battery': np.float32(96.658844), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 111, 'raw_pos': (42.000538176748016, 43.80224951627568), 'raw_vel': (9.835830875994352, 6.930263612820255), 'raw_angle_rad': -1.6361350720033825, 'raw_angular_vel': -11.21713882420815, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (20.57, 38.81) m\n",
      "Environment Reset. Wind Dir: 198.9 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 50 Starting ---\n",
      "Step 84: LOST CONTROL!\n",
      "--- Episode 50 Finished ---\n",
      "Steps: 85, Total Reward: -40.46\n",
      "Final Info: {'Battery': np.float32(97.45628), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 85, 'raw_pos': (22.19835514390857, 42.716630410506816), 'raw_vel': (4.201415142988358, 4.42935394024024), 'raw_angle_rad': -1.725441364321574, 'raw_angular_vel': -9.607848318818608, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (23.11, 33.38) m\n",
      "Environment Reset. Wind Dir: 151.3 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 51 Starting ---\n",
      "Step 113: OUT OF BOUNDS!\n",
      "--- Episode 51 Finished ---\n",
      "Steps: 114, Total Reward: -36.00\n",
      "Final Info: {'Battery': np.float32(96.52992), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 114, 'raw_pos': (22.586669130569028, 50.04385096264376), 'raw_vel': (-0.8091143378964037, 14.566512368275967), 'raw_angle_rad': -0.2522865998838893, 'raw_angular_vel': -2.5052110114648, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (27.61, 40.92) m\n",
      "Environment Reset. Wind Dir: 137.5 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 52 Starting ---\n",
      "Step 91: OUT OF BOUNDS!\n",
      "--- Episode 52 Finished ---\n",
      "Steps: 92, Total Reward: -38.99\n",
      "Final Info: {'Battery': np.float32(97.39819), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 92, 'raw_pos': (21.11010659685192, 50.15578386008021), 'raw_vel': (-8.09746436990361, 10.492348177818974), 'raw_angle_rad': -0.0984881792611344, 'raw_angular_vel': -1.6896340241695922, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (32.46, 32.10) m\n",
      "Environment Reset. Wind Dir: 334.4 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 53 Starting ---\n",
      "Step 136: LOST CONTROL!\n",
      "--- Episode 53 Finished ---\n",
      "Steps: 137, Total Reward: -33.89\n",
      "Final Info: {'Battery': np.float32(96.0815), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 137, 'raw_pos': (38.68293121303001, 44.16108216142677), 'raw_vel': (7.655054119168178, 6.882554736410382), 'raw_angle_rad': -1.68881800731801, 'raw_angular_vel': -10.701142071876728, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (23.11, 40.23) m\n",
      "Environment Reset. Wind Dir: 270.2 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 54 Starting ---\n",
      "Step 102: LOST CONTROL!\n",
      "--- Episode 54 Finished ---\n",
      "Steps: 103, Total Reward: -38.65\n",
      "Final Info: {'Battery': np.float32(97.036934), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 103, 'raw_pos': (16.51146602525477, 44.35022991182703), 'raw_vel': (-3.855195177716326, 1.81413164394616), 'raw_angle_rad': -1.8017526672629822, 'raw_angular_vel': -16.59509841748158, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (20.80, 39.60) m\n",
      "Environment Reset. Wind Dir: 355.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 55 Starting ---\n",
      "Step 109: OUT OF BOUNDS!\n",
      "--- Episode 55 Finished ---\n",
      "Steps: 110, Total Reward: -38.56\n",
      "Final Info: {'Battery': np.float32(96.85772), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 110, 'raw_pos': (32.278480314071196, 50.148293842112736), 'raw_vel': (12.071590787629711, 8.166092155088785), 'raw_angle_rad': -0.839076845176073, 'raw_angular_vel': -2.808916349973741, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n",
      "Drone starting at: (32.33, 44.33) m\n",
      "Environment Reset. Wind Dir: 145.1 deg. Moving Platform: False\n",
      "\n",
      "--- Episode 56 Starting ---\n",
      "Step 68: OUT OF BOUNDS!\n",
      "--- Episode 56 Finished ---\n",
      "Steps: 69, Total Reward: -41.84\n",
      "Final Info: {'Battery': np.float32(97.975334), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 69, 'raw_pos': (28.79299350594019, 50.1127952155038), 'raw_vel': (-3.9981701391464304, 8.35786700197838), 'raw_angle_rad': 0.10843406463158584, 'raw_angular_vel': 3.0093054044805383, 'platform_pos_x': 25.0, 'platform_vel_x': 0.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnvironment Closed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     \u001b[43mrun_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 66\u001b[0m, in \u001b[0;36mrun_env\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m RENDER:\n\u001b[1;32m     64\u001b[0m                 \u001b[38;5;66;03m# Keep window open briefly after episode ends\u001b[39;00m\n\u001b[1;32m     65\u001b[0m                 env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m---> 66\u001b[0m                 \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnvironment Closed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Assuming your package is installed or files are in the right place\n",
    "# Replace with your actual import path if needed\n",
    "from drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "import time\n",
    "\n",
    "# --- Configuration Options ---\n",
    "RENDER = True\n",
    "MAX_EPISODE_STEPS = 1500\n",
    "MOVING_PLATFORM = False\n",
    "PLATFORM_SPEED = 2.5\n",
    "INITIAL_RANDOM_RANGE = 9.0 # +/- 10m range\n",
    "\n",
    "def run_env():\n",
    "    # Initialize environment with desired options\n",
    "    env = Drone2dEnv(\n",
    "        render_sim=RENDER,\n",
    "        max_steps=MAX_EPISODE_STEPS,\n",
    "        moving_platform=MOVING_PLATFORM,\n",
    "        platform_speed=PLATFORM_SPEED,\n",
    "        initial_pos_random_range_m=INITIAL_RANDOM_RANGE\n",
    "    )\n",
    "\n",
    "    print(\"Action Space:\", env.action_space)\n",
    "    print(\"Observation Space:\", env.observation_space)\n",
    "    print(\"Observation Space Sample:\", env.observation_space.sample())\n",
    "\n",
    "    # Run a few episodes\n",
    "    for episode in range(100):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        print(f\"\\n--- Episode {episode + 1} Starting ---\")\n",
    "\n",
    "        while not done:\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "                # Add a small delay to make rendering watchable\n",
    "                # time.sleep(0.01)\n",
    "\n",
    "            # --- Replace with your RL agent's action selection ---\n",
    "            # action = agent.predict(obs) # Example placeholder\n",
    "            action = env.action_space.sample() # Take random actions for now\n",
    "            # ---\n",
    "\n",
    "            # Step the environment\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "\n",
    "            # Optional: Print step info\n",
    "            # if step_count % 50 == 0:\n",
    "            #     print(f\"Step: {step_count}, Reward: {reward:.3f}, Done: {done}\")\n",
    "            #     # print(f\"Obs: {[f'{x:.2f}' for x in obs]}\")\n",
    "            #     # print(f\"Info: {info}\")\n",
    "\n",
    "            if done:\n",
    "                print(f\"--- Episode {episode + 1} Finished ---\")\n",
    "                print(f\"Steps: {step_count}, Total Reward: {total_reward:.2f}\")\n",
    "                print(f\"Final Info: {info}\")\n",
    "                if RENDER:\n",
    "                    # Keep window open briefly after episode ends\n",
    "                    env.render()\n",
    "                    time.sleep(1.5)\n",
    "\n",
    "    env.close()\n",
    "    print(\"\\nEnvironment Closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Drone Environment Test ---\n",
      "Observation Space Size: (12,)\n",
      "Drone starting at: (32.02, 49.80) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 111.4 deg. Moving Platform: True\n",
      "\n",
      "--- Environment Initialized ---\n",
      "Action Space: Box(-1.0, 1.0, (2,), float32)\n",
      "Observation Space: Box([-1. -1. -1. -1. -1. -1.  0. -1. -1. -1.  0. -1.], 1.0, (12,), float32)\n",
      "\n",
      "--- Starting Episode 1/5 ---\n",
      "Drone starting at: (18.67, 35.13) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 350.2 deg. Moving Platform: True\n",
      "Initial Observation sample: [-0.25318128  0.40503347  0.          0.        ]...\n",
      "Step 119: OUT OF BOUNDS!\n",
      "--- Episode 1 Finished ---\n",
      "Steps taken: 120\n",
      "Total Reward: -40.41\n",
      "Final Info: {'Battery': np.float32(96.22567), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 120, 'raw_pos': (34.949240622638804, 50.034143421290686), 'raw_vel': (12.05444134892994, 12.201254054817994), 'raw_angle_rad': 0.04105185285085045, 'raw_angular_vel': -0.7048481064942187, 'platform_pos_x': 18.999999999999915, 'platform_vel_x': -2.5}\n",
      "\n",
      "--- Starting Episode 2/5 ---\n",
      "Drone starting at: (22.01, 40.19) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 185.3 deg. Moving Platform: True\n",
      "Initial Observation sample: [-0.11970533  0.607407    0.          0.        ]...\n",
      "Step 108: OUT OF BOUNDS!\n",
      "--- Episode 2 Finished ---\n",
      "Steps taken: 109\n",
      "Total Reward: -38.77\n",
      "Final Info: {'Battery': np.float32(96.90944), 'landed': False, 'crashed': False, 'out_of_bounds': True, 'Battery_empty': False, 'lost_control': False, 'steps': 109, 'raw_pos': (16.83723705367745, 50.087958888810775), 'raw_vel': (-5.178593286048371, 7.92274603285796), 'raw_angle_rad': 1.0916001624444467, 'raw_angular_vel': 6.121258649185048, 'platform_pos_x': 30.450000000000077, 'platform_vel_x': 2.5}\n",
      "\n",
      "--- Starting Episode 3/5 ---\n",
      "Drone starting at: (15.54, 41.65) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 194.8 deg. Moving Platform: True\n",
      "Initial Observation sample: [-0.37828964  0.66590947  0.          0.        ]...\n",
      "Step 69: LOST CONTROL!\n",
      "--- Episode 3 Finished ---\n",
      "Steps taken: 70\n",
      "Total Reward: -43.34\n",
      "Final Info: {'Battery': np.float32(97.941154), 'landed': False, 'crashed': False, 'out_of_bounds': False, 'Battery_empty': False, 'lost_control': True, 'steps': 70, 'raw_pos': (12.677324566234848, 46.09388539316906), 'raw_vel': (-0.7543184999380976, 4.0141990353203125), 'raw_angle_rad': -1.5938415818570593, 'raw_angular_vel': -7.293623677233384, 'platform_pos_x': 21.49999999999995, 'platform_vel_x': -2.5}\n",
      "\n",
      "--- Starting Episode 4/5 ---\n",
      "Drone starting at: (17.17, 39.41) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 299.5 deg. Moving Platform: True\n",
      "Initial Observation sample: [-0.31332657  0.5765179   0.          0.        ]...\n"
     ]
    }
   ],
   "source": [
    "# --- START OF FILE test_drone_env.py ---\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# IMPORTANT: Adjust this import path based on how your project is structured\n",
    "# and how you installed your custom environment.\n",
    "# Option 1: If installed as a package\n",
    "# from drone_2d_custom_gym_env_package.drone_2d_custom_gym_env import Drone2dEnv\n",
    "# Option 2: If drone_2d_env.py is in the same directory or accessible via PYTHONPATH\n",
    "try:\n",
    "    from drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "except ImportError:\n",
    "    print(\"Error importing Drone2dEnv. Make sure the environment file is accessible.\")\n",
    "    print(\"Attempting relative import...\")\n",
    "    # Adjust relative path if needed (e.g., if test file is outside the package)\n",
    "    from drone_2d_custom_gym_env_package.drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "\n",
    "\n",
    "# --- Test Configuration ---\n",
    "NUM_EPISODES = 5       # How many episodes to run\n",
    "MAX_STEPS_PER_EPISODE = 750 # Max steps before terminating an episode\n",
    "RENDER_SIM = True      # Set to True to visualize the simulation\n",
    "RENDER_DELAY_S = 0.02  # Small delay (in seconds) between frames if rendering\n",
    "\n",
    "# --- Environment Options ---\n",
    "# Choose the configuration you want to test\n",
    "ENABLE_WIND_TEST = True\n",
    "MOVING_PLATFORM_TEST = True\n",
    "PLATFORM_SPEED_TEST = 2.5 # Meters per second\n",
    "INITIAL_RANDOM_RANGE_TEST = 10.0 # Meters (+/- from center)\n",
    "MAX_TILT_ANGLE_DEG_TEST = 90.0 # Degrees\n",
    "\n",
    "def run_test():\n",
    "    \"\"\"Runs the test loop for the Drone2dEnv.\"\"\"\n",
    "    print(\"--- Starting Drone Environment Test ---\")\n",
    "\n",
    "    # Convert degrees to radians for the environment parameter\n",
    "    max_tilt_rad = np.radians(MAX_TILT_ANGLE_DEG_TEST)\n",
    "\n",
    "    # Initialize the environment with chosen options\n",
    "    try:\n",
    "        env = Drone2dEnv(\n",
    "            render_sim=RENDER_SIM,\n",
    "            max_steps=MAX_STEPS_PER_EPISODE,\n",
    "            moving_platform=MOVING_PLATFORM_TEST,\n",
    "            platform_speed=PLATFORM_SPEED_TEST,\n",
    "            initial_pos_random_range_m=INITIAL_RANDOM_RANGE_TEST,\n",
    "            enable_wind=ENABLE_WIND_TEST,\n",
    "            max_allowed_tilt_angle_rad=max_tilt_rad,\n",
    "            # Keep other render options (path, shade) as their defaults or specify them\n",
    "            # render_path=True,\n",
    "            # render_shade=True,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error creating environment: {e} !!!\")\n",
    "        print(\"Please check the environment's __init__ method and parameters.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Environment Initialized ---\")\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    # print(f\"Sample Observation: {env.observation_space.sample()}\") # Can be useful for checking bounds\n",
    "\n",
    "    total_steps_all_episodes = 0\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        print(f\"\\n--- Starting Episode {episode + 1}/{NUM_EPISODES} ---\")\n",
    "        try:\n",
    "            # Reset the environment for a new episode\n",
    "            obs = env.reset()\n",
    "            print(f\"Initial Observation sample: {obs[:4]}...\") # Print start of obs\n",
    "        except Exception as e:\n",
    "            print(f\"\\n!!! Error resetting environment: {e} !!!\")\n",
    "            print(\"Skipping episode.\")\n",
    "            continue\n",
    "\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            # Render the current state (if enabled)\n",
    "            if RENDER_SIM:\n",
    "                try:\n",
    "                    env.render()\n",
    "                    if RENDER_DELAY_S > 0:\n",
    "                        time.sleep(RENDER_DELAY_S)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n!!! Error rendering environment: {e} !!!\")\n",
    "                    print(\"Disabling rendering for this episode.\")\n",
    "                    # Optionally disable rendering completely: RENDER_SIM = False\n",
    "                    break # Stop this episode if rendering fails badly\n",
    "\n",
    "            # --- Action Selection ---\n",
    "            # For testing, use random actions. Replace with your agent's policy later.\n",
    "            action = env.action_space.sample()\n",
    "            # print(f\"Step {step_count}: Action = {action}\") # Uncomment for debugging action values\n",
    "\n",
    "            # --- Step the Environment ---\n",
    "            try:\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                total_reward += reward\n",
    "                step_count += 1\n",
    "                total_steps_all_episodes += 1\n",
    "            except Exception as e:\n",
    "                print(f\"\\n!!! Error during env.step() at step {step_count}: {e} !!!\")\n",
    "                print(\"Terminating episode.\")\n",
    "                done = True # Force episode termination\n",
    "                info = info if 'info' in locals() else {} # Use existing info if available\n",
    "                info['error'] = f\"Exception during step: {e}\"\n",
    "\n",
    "            # Optional: Print step info periodically\n",
    "            # if step_count % 100 == 0:\n",
    "            #     print(f\"  Step: {step_count}, Reward: {reward:.3f}, Done: {done}\")\n",
    "\n",
    "            # Check if max steps reached (env should handle this with 'done', but as a backup)\n",
    "            if step_count >= MAX_STEPS_PER_EPISODE and not done:\n",
    "                print(\"Warning: Max steps reached in test loop, but env not 'done'.\")\n",
    "                done = True # Force termination in test script\n",
    "\n",
    "        # --- Episode End ---\n",
    "        print(f\"--- Episode {episode + 1} Finished ---\")\n",
    "        print(f\"Steps taken: {step_count}\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\")\n",
    "        print(f\"Final Info: {info}\")\n",
    "\n",
    "        # Render the final frame after done (if rendering)\n",
    "        if RENDER_SIM:\n",
    "             try: env.render(); time.sleep(0.5) # Pause briefly on final frame\n",
    "             except: pass # Ignore render errors on final frame\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    try:\n",
    "        env.close()\n",
    "        print(\"\\nEnvironment Closed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError closing environment: {e}\")\n",
    "\n",
    "    print(f\"\\n--- Test Finished ---\")\n",
    "    print(f\"Total steps across all episodes: {total_steps_all_episodes}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model (Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Environment Configuration ---\n",
    "# These should match the parameters you want to train the agent on\n",
    "ENV_CONFIG = {\n",
    "    \"render_sim\": False,             # IMPORTANT: Keep False for faster training\n",
    "    \"max_steps\": 750,                # Max steps per episode during training\n",
    "    \"render_path\": False,            # Disable rendering options for speed\n",
    "    \"render_shade\": False,\n",
    "    \"shade_distance_m\": 2.0,\n",
    "             \n",
    "    # No change if we are going resume training training\n",
    "    \n",
    "    \"moving_platform\": True,        # Example: Train with moving platform\n",
    "    \"platform_speed\": 2.0,           # Example: Platform speed if moving\n",
    "    \"initial_pos_random_range_m\": 8.0,# Example: Random start range\n",
    "    \"max_allowed_tilt_angle_rad\": 1.5, # Example: Allow slightly more tilt (approx 86 deg)\n",
    "    \"enable_wind\": True,             # Example: Train with wind enabled\n",
    "    # Note: wind_speed is fixed at 5.0 inside the env for now\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Drone Training ---\n",
      "Algorithm: PPO\n",
      "Total Timesteps: 1800000\n",
      "Environment Config: {'render_sim': False, 'max_steps': 750, 'render_path': False, 'render_shade': False, 'shade_distance_m': 2.0, 'moving_platform': True, 'platform_speed': 2.0, 'initial_pos_random_range_m': 8.0, 'max_allowed_tilt_angle_rad': 1.5, 'enable_wind': True}\n",
      "Using 4 parallel environments.\n",
      "Observation Space Size: (12,)\n",
      "Drone starting at: (31.86, 36.49) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 326.1 deg. Moving Platform: True\n",
      "Observation Space Size: (12,)\n",
      "Drone starting at: (32.87, 38.74) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 67.9 deg. Moving Platform: True\n",
      "Observation Space Size: (12,)\n",
      "Drone starting at: (25.68, 38.51) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 250.7 deg. Moving Platform: True\n",
      "Observation Space Size: (12,)\n",
      "Drone starting at: (27.73, 37.22) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 186.3 deg. Moving Platform: True\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebas_dev_linux/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/sebas_dev_linux/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/sebas_dev_linux/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/sebas_dev_linux/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Drone starting at: (17.26, 45.01) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 14.8 deg. Moving Platform: True\n",
      "Drone starting at: (32.18, 36.99) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 51.9 deg. Moving Platform: True\n",
      "Drone starting at: (18.47, 41.60) m\n",
      "Drone starting at: (26.31, 33.51) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 293.1 deg. Moving Platform: TrueEnvironment Reset. Wind Enabled: True, Wind Dir: 288.5 deg. Moving Platform: True\n",
      "\n",
      "Logging to logs/drone_ppo/PPO_Drone_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebas_dev_linux/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 60: OUT OF BOUNDS!\n",
      "Drone starting at: (25.97, 36.61) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 199.5 deg. Moving Platform: True\n",
      "Step 104: LOST CONTROL!\n",
      "Drone starting at: (19.47, 40.16) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 57.1 deg. Moving Platform: True\n",
      "Step 110: LOST CONTROL!\n",
      "Drone starting at: (19.69, 41.86) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 76.4 deg. Moving Platform: True\n",
      "Step 58: LOST CONTROL!\n",
      "Drone starting at: (18.39, 43.33) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 315.9 deg. Moving Platform: True\n",
      "Step 120: LOST CONTROL!\n",
      "Drone starting at: (21.28, 43.74) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 99.8 deg. Moving Platform: True\n",
      "Step 61: OUT OF BOUNDS!\n",
      "Drone starting at: (31.51, 35.78) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 4.9 deg. Moving Platform: True\n",
      "Step 75: OUT OF BOUNDS!\n",
      "Drone starting at: (26.53, 43.72) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 26.9 deg. Moving Platform: True\n",
      "Step 82: OUT OF BOUNDS!\n",
      "Drone starting at: (23.50, 36.80) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 355.5 deg. Moving Platform: True\n",
      "Step 101: OUT OF BOUNDS!\n",
      "Drone starting at: (18.20, 40.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 293.6 deg. Moving Platform: True\n",
      "Step 67: LOST CONTROL!\n",
      "Drone starting at: (25.27, 46.86) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 14.2 deg. Moving Platform: True\n",
      "Step 72: LOST CONTROL!\n",
      "Drone starting at: (29.61, 40.03) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 259.1 deg. Moving Platform: True\n",
      "Step 82: LOST CONTROL!\n",
      "Drone starting at: (20.38, 36.12) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 184.2 deg. Moving Platform: True\n",
      "Step 64: LOST CONTROL!\n",
      "Drone starting at: (23.05, 33.90) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 151.5 deg. Moving Platform: True\n",
      "Step 58: OUT OF BOUNDS!\n",
      "Drone starting at: (24.19, 43.81) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 332.1 deg. Moving Platform: True\n",
      "Step 97: LOST CONTROL!\n",
      "Drone starting at: (30.95, 36.35) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 256.2 deg. Moving Platform: True\n",
      "Step 96: LOST CONTROL!\n",
      "Drone starting at: (20.31, 44.58) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 164.3 deg. Moving Platform: True\n",
      "Step 76: LOST CONTROL!\n",
      "Drone starting at: (28.78, 41.48) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 214.4 deg. Moving Platform: True\n",
      "Step 94: OUT OF BOUNDS!\n",
      "Drone starting at: (21.81, 37.91) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 334.4 deg. Moving Platform: True\n",
      "Step 62: LOST CONTROL!\n",
      "Drone starting at: (30.14, 45.24) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 45.5 deg. Moving Platform: True\n",
      "Step 67: OUT OF BOUNDS!\n",
      "Drone starting at: (26.25, 36.30) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 95.5 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (23.43, 39.88) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 245.3 deg. Moving Platform: True\n",
      "Step 61: OUT OF BOUNDS!\n",
      "Drone starting at: (32.99, 41.62) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 226.3 deg. Moving Platform: True\n",
      "Step 55: LOST CONTROL!\n",
      "Drone starting at: (29.19, 35.50) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 175.1 deg. Moving Platform: True\n",
      "Step 120: OUT OF BOUNDS!\n",
      "Drone starting at: (25.72, 37.31) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 201.8 deg. Moving Platform: True\n",
      "Step 66: LOST CONTROL!\n",
      "Drone starting at: (17.33, 47.19) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 137.4 deg. Moving Platform: True\n",
      "Step 41: OUT OF BOUNDS!\n",
      "Drone starting at: (30.86, 37.28) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 10.3 deg. Moving Platform: True\n",
      "Step 106: LOST CONTROL!\n",
      "Drone starting at: (28.79, 44.66) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 170.3 deg. Moving Platform: True\n",
      "Step 152: LOST CONTROL!\n",
      "Drone starting at: (22.81, 35.06) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 196.0 deg. Moving Platform: True\n",
      "Step 92: LOST CONTROL!\n",
      "Drone starting at: (30.42, 46.72) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 229.7 deg. Moving Platform: True\n",
      "Step 66: LOST CONTROL!\n",
      "Drone starting at: (24.36, 40.89) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 208.2 deg. Moving Platform: True\n",
      "Step 66: LOST CONTROL!\n",
      "Drone starting at: (32.32, 46.46) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 186.2 deg. Moving Platform: True\n",
      "Step 62: OUT OF BOUNDS!\n",
      "Drone starting at: (31.38, 36.86) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 51.3 deg. Moving Platform: True\n",
      "Step 64: OUT OF BOUNDS!\n",
      "Drone starting at: (18.26, 44.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 193.4 deg. Moving Platform: True\n",
      "Step 137: OUT OF BOUNDS!\n",
      "Drone starting at: (25.13, 42.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 135.7 deg. Moving Platform: True\n",
      "Step 96: LOST CONTROL!\n",
      "Drone starting at: (32.60, 41.21) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 60.9 deg. Moving Platform: True\n",
      "Step 96: OUT OF BOUNDS!\n",
      "Drone starting at: (30.92, 34.02) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 107.1 deg. Moving Platform: True\n",
      "Step 85: OUT OF BOUNDS!\n",
      "Drone starting at: (17.84, 33.74) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 231.8 deg. Moving Platform: True\n",
      "Step 84: OUT OF BOUNDS!\n",
      "Drone starting at: (31.33, 33.37) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 154.9 deg. Moving Platform: True\n",
      "Step 77: OUT OF BOUNDS!\n",
      "Drone starting at: (27.70, 39.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 85.7 deg. Moving Platform: True\n",
      "Step 107: OUT OF BOUNDS!\n",
      "Drone starting at: (31.40, 36.41) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 162.2 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (22.21, 34.17) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 136.9 deg. Moving Platform: True\n",
      "Step 74: LOST CONTROL!\n",
      "Drone starting at: (29.05, 41.23) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 305.1 deg. Moving Platform: True\n",
      "Step 89: LOST CONTROL!\n",
      "Drone starting at: (30.47, 39.10) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 338.4 deg. Moving Platform: True\n",
      "Step 111: LOST CONTROL!\n",
      "Drone starting at: (25.03, 39.08) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 327.5 deg. Moving Platform: True\n",
      "Step 94: LOST CONTROL!\n",
      "Drone starting at: (22.03, 34.99) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 2.6 deg. Moving Platform: True\n",
      "Step 108: OUT OF BOUNDS!\n",
      "Drone starting at: (18.09, 44.60) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 110.1 deg. Moving Platform: True\n",
      "Step 121: OUT OF BOUNDS!\n",
      "Drone starting at: (18.56, 43.01) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 16.1 deg. Moving Platform: True\n",
      "Step 60: OUT OF BOUNDS!\n",
      "Drone starting at: (29.41, 34.14) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 45.3 deg. Moving Platform: True\n",
      "Step 81: LOST CONTROL!\n",
      "Drone starting at: (32.58, 44.91) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 72.7 deg. Moving Platform: True\n",
      "Step 93: LOST CONTROL!\n",
      "Drone starting at: (27.29, 44.96) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 67.2 deg. Moving Platform: True\n",
      "Step 73: LOST CONTROL!\n",
      "Drone starting at: (24.01, 40.87) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 80.3 deg. Moving Platform: True\n",
      "Step 62: OUT OF BOUNDS!\n",
      "Drone starting at: (24.17, 46.87) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 24.3 deg. Moving Platform: True\n",
      "Step 65: OUT OF BOUNDS!\n",
      "Drone starting at: (29.04, 44.00) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 134.1 deg. Moving Platform: True\n",
      "Step 97: OUT OF BOUNDS!\n",
      "Drone starting at: (24.14, 44.58) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 279.0 deg. Moving Platform: True\n",
      "Step 75: OUT OF BOUNDS!\n",
      "Drone starting at: (29.41, 34.26) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 120.6 deg. Moving Platform: True\n",
      "Step 53: OUT OF BOUNDS!\n",
      "Drone starting at: (32.68, 46.62) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 177.2 deg. Moving Platform: True\n",
      "Step 74: OUT OF BOUNDS!\n",
      "Drone starting at: (17.31, 47.73) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 18.2 deg. Moving Platform: True\n",
      "Step 67: LOST CONTROL!\n",
      "Drone starting at: (28.39, 34.87) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 167.4 deg. Moving Platform: True\n",
      "Step 96: OUT OF BOUNDS!\n",
      "Drone starting at: (18.92, 46.40) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 174.0 deg. Moving Platform: True\n",
      "Step 59: OUT OF BOUNDS!\n",
      "Drone starting at: (30.20, 41.87) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 57.1 deg. Moving Platform: True\n",
      "Step 42: OUT OF BOUNDS!\n",
      "Drone starting at: (17.14, 34.24) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 216.8 deg. Moving Platform: True\n",
      "Step 52: OUT OF BOUNDS!\n",
      "Drone starting at: (28.28, 45.06) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 310.2 deg. Moving Platform: True\n",
      "Step 69: LOST CONTROL!\n",
      "Drone starting at: (29.69, 46.06) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 113.5 deg. Moving Platform: True\n",
      "Step 72: LOST CONTROL!\n",
      "Drone starting at: (32.87, 38.80) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 165.0 deg. Moving Platform: True\n",
      "Step 73: LOST CONTROL!\n",
      "Drone starting at: (24.14, 37.59) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 188.3 deg. Moving Platform: True\n",
      "Step 57: OUT OF BOUNDS!\n",
      "Drone starting at: (30.42, 44.62) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 52.9 deg. Moving Platform: True\n",
      "Step 96: LOST CONTROL!\n",
      "Drone starting at: (17.12, 42.74) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 337.2 deg. Moving Platform: True\n",
      "Step 88: LOST CONTROL!\n",
      "Drone starting at: (17.08, 33.84) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 195.2 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (22.10, 46.01) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 329.8 deg. Moving Platform: True\n",
      "Step 65: OUT OF BOUNDS!\n",
      "Drone starting at: (20.05, 36.85) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 13.6 deg. Moving Platform: True\n",
      "Step 77: LOST CONTROL!\n",
      "Drone starting at: (19.23, 45.79) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 51.4 deg. Moving Platform: True\n",
      "Step 65: LOST CONTROL!\n",
      "Drone starting at: (27.10, 40.83) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 222.3 deg. Moving Platform: True\n",
      "Step 67: LOST CONTROL!\n",
      "Drone starting at: (23.28, 45.47) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 264.5 deg. Moving Platform: True\n",
      "Step 91: LOST CONTROL!\n",
      "Drone starting at: (30.13, 45.89) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 150.1 deg. Moving Platform: True\n",
      "Step 59: OUT OF BOUNDS!\n",
      "Drone starting at: (22.77, 43.30) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 148.2 deg. Moving Platform: True\n",
      "Step 66: LOST CONTROL!\n",
      "Drone starting at: (24.64, 46.74) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 40.0 deg. Moving Platform: True\n",
      "Step 63: OUT OF BOUNDS!\n",
      "Drone starting at: (24.10, 45.96) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 144.5 deg. Moving Platform: True\n",
      "Step 124: OUT OF BOUNDS!\n",
      "Drone starting at: (32.42, 40.65) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 266.9 deg. Moving Platform: True\n",
      "Step 51: OUT OF BOUNDS!\n",
      "Drone starting at: (27.53, 46.79) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 129.5 deg. Moving Platform: True\n",
      "Step 77: OUT OF BOUNDS!\n",
      "Drone starting at: (32.53, 33.92) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 352.4 deg. Moving Platform: True\n",
      "Step 41: LOST CONTROL!\n",
      "Drone starting at: (29.36, 39.84) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 218.0 deg. Moving Platform: True\n",
      "Step 47: OUT OF BOUNDS!\n",
      "Drone starting at: (29.27, 36.41) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 211.5 deg. Moving Platform: True\n",
      "Step 59: LOST CONTROL!\n",
      "Drone starting at: (22.89, 37.93) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 119.3 deg. Moving Platform: True\n",
      "Step 84: LOST CONTROL!\n",
      "Drone starting at: (27.60, 42.35) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 35.1 deg. Moving Platform: True\n",
      "Step 94: LOST CONTROL!\n",
      "Drone starting at: (25.48, 34.05) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 254.3 deg. Moving Platform: True\n",
      "Step 87: LOST CONTROL!\n",
      "Drone starting at: (20.22, 33.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 82.5 deg. Moving Platform: True\n",
      "Step 86: LOST CONTROL!\n",
      "Drone starting at: (22.63, 36.93) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 232.2 deg. Moving Platform: True\n",
      "Step 89: OUT OF BOUNDS!\n",
      "Drone starting at: (30.92, 41.62) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 1.8 deg. Moving Platform: True\n",
      "Step 94: LOST CONTROL!\n",
      "Drone starting at: (26.20, 33.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 203.1 deg. Moving Platform: True\n",
      "Step 72: LOST CONTROL!\n",
      "Drone starting at: (19.67, 32.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 186.4 deg. Moving Platform: True\n",
      "Step 98: OUT OF BOUNDS!\n",
      "Drone starting at: (21.69, 46.31) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 338.3 deg. Moving Platform: True\n",
      "Step 105: LOST CONTROL!\n",
      "Drone starting at: (26.81, 45.84) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 97.2 deg. Moving Platform: True\n",
      "Step 84: LOST CONTROL!\n",
      "Drone starting at: (29.67, 44.09) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 329.8 deg. Moving Platform: True\n",
      "Step 111: LOST CONTROL!\n",
      "Drone starting at: (25.26, 37.40) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 299.7 deg. Moving Platform: True\n",
      "Step 52: OUT OF BOUNDS!\n",
      "Drone starting at: (22.05, 40.63) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 200.8 deg. Moving Platform: True\n",
      "Step 115: LOST CONTROL!\n",
      "Drone starting at: (19.09, 42.75) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 169.1 deg. Moving Platform: True\n",
      "Step 59: LOST CONTROL!\n",
      "Drone starting at: (21.28, 43.46) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 265.5 deg. Moving Platform: True\n",
      "Step 75: LOST CONTROL!\n",
      "Drone starting at: (24.70, 34.18) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 108.5 deg. Moving Platform: True\n",
      "Step 88: OUT OF BOUNDS!\n",
      "Drone starting at: (20.36, 32.91) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 17.4 deg. Moving Platform: True\n",
      "Step 114: LOST CONTROL!\n",
      "Drone starting at: (32.12, 35.92) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 149.5 deg. Moving Platform: True\n",
      "Step 80: LOST CONTROL!\n",
      "Drone starting at: (21.08, 44.98) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 240.2 deg. Moving Platform: True\n",
      "Step 49: LOST CONTROL!\n",
      "Drone starting at: (32.67, 45.14) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 277.6 deg. Moving Platform: True\n",
      "Step 99: LOST CONTROL!\n",
      "Drone starting at: (21.48, 45.72) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 253.4 deg. Moving Platform: True\n",
      "Step 72: OUT OF BOUNDS!\n",
      "Drone starting at: (28.17, 32.06) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 260.2 deg. Moving Platform: True\n",
      "Step 116: OUT OF BOUNDS!\n",
      "Drone starting at: (22.89, 39.03) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 228.5 deg. Moving Platform: True\n",
      "Step 93: LOST CONTROL!\n",
      "Drone starting at: (23.33, 47.86) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 68.8 deg. Moving Platform: True\n",
      "Step 73: LOST CONTROL!\n",
      "Drone starting at: (21.21, 39.72) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 108.0 deg. Moving Platform: True\n",
      "Step 42: OUT OF BOUNDS!\n",
      "Drone starting at: (29.53, 43.05) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 93.1 deg. Moving Platform: True\n",
      "Step 119: LOST CONTROL!\n",
      "Drone starting at: (31.33, 45.45) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 115.5 deg. Moving Platform: True\n",
      "Step 90: LOST CONTROL!\n",
      "Drone starting at: (25.51, 44.73) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 234.2 deg. Moving Platform: True\n",
      "Step 56: OUT OF BOUNDS!\n",
      "Drone starting at: (30.17, 42.50) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 327.4 deg. Moving Platform: True\n",
      "Step 72: OUT OF BOUNDS!\n",
      "Drone starting at: (23.80, 43.98) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 43.6 deg. Moving Platform: True\n",
      "Step 86: OUT OF BOUNDS!\n",
      "Drone starting at: (17.48, 41.31) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 26.1 deg. Moving Platform: True\n",
      "Step 72: OUT OF BOUNDS!\n",
      "Drone starting at: (31.75, 47.24) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 294.0 deg. Moving Platform: True\n",
      "Step 87: LOST CONTROL!\n",
      "Drone starting at: (22.75, 40.91) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 66.1 deg. Moving Platform: True\n",
      "Step 88: OUT OF BOUNDS!\n",
      "Drone starting at: (32.58, 34.39) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 4.9 deg. Moving Platform: True\n",
      "Step 161: OUT OF BOUNDS!\n",
      "Drone starting at: (29.52, 45.38) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 112.8 deg. Moving Platform: True\n",
      "Step 75: LOST CONTROL!\n",
      "Drone starting at: (31.98, 35.76) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 98.1 deg. Moving Platform: True\n",
      "Step 81: LOST CONTROL!\n",
      "Drone starting at: (20.33, 33.60) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 246.3 deg. Moving Platform: True\n",
      "Step 61: OUT OF BOUNDS!\n",
      "Drone starting at: (17.32, 35.21) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 5.1 deg. Moving Platform: True\n",
      "Step 103: LOST CONTROL!\n",
      "Drone starting at: (30.18, 39.51) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 287.1 deg. Moving Platform: True\n",
      "Step 67: LOST CONTROL!\n",
      "Drone starting at: (19.40, 45.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 196.5 deg. Moving Platform: True\n",
      "Step 93: LOST CONTROL!\n",
      "Drone starting at: (17.17, 32.29) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 92.5 deg. Moving Platform: True\n",
      "Step 43: LOST CONTROL!\n",
      "Drone starting at: (19.48, 44.85) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 71.4 deg. Moving Platform: True\n",
      "Step 79: LOST CONTROL!\n",
      "Drone starting at: (22.27, 43.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 285.8 deg. Moving Platform: True\n",
      "Step 116: LOST CONTROL!\n",
      "Drone starting at: (28.08, 32.39) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 26.2 deg. Moving Platform: True\n",
      "Step 73: LOST CONTROL!\n",
      "Drone starting at: (26.84, 47.26) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 129.7 deg. Moving Platform: True\n",
      "Step 60: OUT OF BOUNDS!\n",
      "Drone starting at: (31.28, 45.96) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 143.9 deg. Moving Platform: True\n",
      "Step 41: LOST CONTROL!\n",
      "Drone starting at: (23.14, 47.58) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 350.4 deg. Moving Platform: True\n",
      "Step 48: OUT OF BOUNDS!\n",
      "Drone starting at: (32.24, 33.37) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 80.6 deg. Moving Platform: True\n",
      "Step 98: OUT OF BOUNDS!\n",
      "Drone starting at: (27.67, 45.18) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 11.2 deg. Moving Platform: True\n",
      "Step 48: OUT OF BOUNDS!\n",
      "Drone starting at: (18.75, 37.92) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 58.0 deg. Moving Platform: True\n",
      "Step 58: OUT OF BOUNDS!\n",
      "Drone starting at: (23.10, 46.91) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 205.5 deg. Moving Platform: True\n",
      "Step 74: LOST CONTROL!\n",
      "Drone starting at: (18.63, 33.17) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 311.0 deg. Moving Platform: True\n",
      "Step 74: LOST CONTROL!\n",
      "Drone starting at: (19.55, 40.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 288.4 deg. Moving Platform: True\n",
      "Step 75: LOST CONTROL!Step 72: OUT OF BOUNDS!\n",
      "\n",
      "Drone starting at: (31.72, 37.26) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 260.5 deg. Moving Platform: True\n",
      "Drone starting at: (31.73, 39.87) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 351.6 deg. Moving Platform: True\n",
      "Step 56: LOST CONTROL!\n",
      "Drone starting at: (30.76, 37.15) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 288.1 deg. Moving Platform: True\n",
      "Step 93: LOST CONTROL!\n",
      "Drone starting at: (18.49, 33.66) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 184.2 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (30.26, 36.68) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 288.7 deg. Moving Platform: True\n",
      "Step 106: LOST CONTROL!\n",
      "Drone starting at: (27.71, 45.77) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 128.9 deg. Moving Platform: True\n",
      "Step 56: OUT OF BOUNDS!\n",
      "Drone starting at: (28.76, 34.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 244.3 deg. Moving Platform: True\n",
      "Step 92: LOST CONTROL!\n",
      "Drone starting at: (20.43, 45.98) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 44.4 deg. Moving Platform: True\n",
      "Step 100: LOST CONTROL!\n",
      "Drone starting at: (22.27, 45.33) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 123.5 deg. Moving Platform: True\n",
      "Step 154: LOST CONTROL!\n",
      "Drone starting at: (17.43, 36.60) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 121.6 deg. Moving Platform: True\n",
      "Step 54: OUT OF BOUNDS!\n",
      "Drone starting at: (27.78, 39.58) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 340.1 deg. Moving Platform: True\n",
      "Step 74: LOST CONTROL!\n",
      "Drone starting at: (25.69, 38.41) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 37.6 deg. Moving Platform: True\n",
      "Step 56: OUT OF BOUNDS!\n",
      "Drone starting at: (29.57, 45.78) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 70.4 deg. Moving Platform: True\n",
      "Step 68: LOST CONTROL!\n",
      "Drone starting at: (17.57, 34.75) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 321.4 deg. Moving Platform: True\n",
      "Step 61: LOST CONTROL!\n",
      "Drone starting at: (17.08, 46.41) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 47.5 deg. Moving Platform: True\n",
      "Step 55: OUT OF BOUNDS!\n",
      "Drone starting at: (22.28, 46.94) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 156.3 deg. Moving Platform: True\n",
      "Step 100: LOST CONTROL!\n",
      "Drone starting at: (19.42, 35.84) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 251.4 deg. Moving Platform: True\n",
      "Step 53: OUT OF BOUNDS!\n",
      "Drone starting at: (31.21, 41.36) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 289.7 deg. Moving Platform: True\n",
      "Step 52: OUT OF BOUNDS!\n",
      "Drone starting at: (29.89, 42.93) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 337.5 deg. Moving Platform: True\n",
      "Step 86: LOST CONTROL!\n",
      "Drone starting at: (31.63, 46.64) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 181.4 deg. Moving Platform: True\n",
      "Step 62: LOST CONTROL!\n",
      "Drone starting at: (28.75, 38.91) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 187.6 deg. Moving Platform: True\n",
      "Step 90: LOST CONTROL!\n",
      "Drone starting at: (24.82, 48.00) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 324.3 deg. Moving Platform: True\n",
      "Step 171: LOST CONTROL!\n",
      "Drone starting at: (30.29, 43.20) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 148.8 deg. Moving Platform: True\n",
      "Step 70: OUT OF BOUNDS!\n",
      "Drone starting at: (29.93, 42.30) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 166.7 deg. Moving Platform: True\n",
      "Step 80: LOST CONTROL!\n",
      "Drone starting at: (29.56, 37.19) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 147.6 deg. Moving Platform: True\n",
      "Step 59: OUT OF BOUNDS!\n",
      "Drone starting at: (18.05, 33.21) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 117.3 deg. Moving Platform: True\n",
      "Step 72: OUT OF BOUNDS!\n",
      "Drone starting at: (18.23, 46.48) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 323.6 deg. Moving Platform: True\n",
      "Step 74: LOST CONTROL!\n",
      "Drone starting at: (21.94, 39.67) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 210.5 deg. Moving Platform: True\n",
      "Step 99: OUT OF BOUNDS!\n",
      "Drone starting at: (24.56, 41.00) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 109.3 deg. Moving Platform: True\n",
      "Step 103: OUT OF BOUNDS!\n",
      "Drone starting at: (29.08, 46.68) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 123.7 deg. Moving Platform: True\n",
      "Step 81: OUT OF BOUNDS!\n",
      "Drone starting at: (20.68, 34.81) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 198.1 deg. Moving Platform: True\n",
      "Step 88: LOST CONTROL!\n",
      "Drone starting at: (24.18, 40.01) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 169.2 deg. Moving Platform: True\n",
      "Step 52: OUT OF BOUNDS!\n",
      "Drone starting at: (19.93, 36.50) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 137.8 deg. Moving Platform: True\n",
      "Step 81: OUT OF BOUNDS!\n",
      "Drone starting at: (27.53, 44.95) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 155.5 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (25.07, 39.53) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 245.4 deg. Moving Platform: True\n",
      "Step 79: LOST CONTROL!\n",
      "Drone starting at: (29.07, 34.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 101.4 deg. Moving Platform: True\n",
      "Step 105: OUT OF BOUNDS!\n",
      "Drone starting at: (26.19, 34.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 112.0 deg. Moving Platform: True\n",
      "Step 62: LOST CONTROL!\n",
      "Step 77: OUT OF BOUNDS!\n",
      "Drone starting at: (24.71, 33.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 254.7 deg. Moving Platform: True\n",
      "Drone starting at: (31.20, 42.79) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 282.8 deg. Moving Platform: True\n",
      "Step 65: LOST CONTROL!\n",
      "Drone starting at: (30.01, 40.85) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 298.1 deg. Moving Platform: True\n",
      "Step 97: OUT OF BOUNDS!\n",
      "Drone starting at: (29.34, 33.00) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 132.7 deg. Moving Platform: True\n",
      "Step 91: LOST CONTROL!\n",
      "Step 101: OUT OF BOUNDS!\n",
      "Drone starting at: (23.30, 44.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 130.6 deg. Moving Platform: True\n",
      "Drone starting at: (23.98, 43.69) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 249.8 deg. Moving Platform: True\n",
      "Step 56: LOST CONTROL!\n",
      "Drone starting at: (19.84, 34.52) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 6.0 deg. Moving Platform: True\n",
      "Step 67: OUT OF BOUNDS!\n",
      "Drone starting at: (24.32, 45.66) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 72.7 deg. Moving Platform: True\n",
      "Step 84: LOST CONTROL!\n",
      "Drone starting at: (25.18, 36.54) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 127.1 deg. Moving Platform: True\n",
      "Step 48: LOST CONTROL!\n",
      "Drone starting at: (28.85, 44.56) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 191.2 deg. Moving Platform: True\n",
      "Step 53: OUT OF BOUNDS!\n",
      "Step 90: LOST CONTROL!\n",
      "Drone starting at: (29.19, 46.17) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 216.0 deg. Moving Platform: True\n",
      "Drone starting at: (28.72, 40.59) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 198.1 deg. Moving Platform: True\n",
      "Step 125: OUT OF BOUNDS!\n",
      "Drone starting at: (19.35, 42.63) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 117.4 deg. Moving Platform: True\n",
      "Step 63: OUT OF BOUNDS!\n",
      "Drone starting at: (27.05, 37.65) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 137.9 deg. Moving Platform: True\n",
      "Step 70: LOST CONTROL!\n",
      "Drone starting at: (17.92, 44.98) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 83.0 deg. Moving Platform: True\n",
      "Step 71: LOST CONTROL!\n",
      "Drone starting at: (19.20, 36.64) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 178.7 deg. Moving Platform: True\n",
      "Step 71: OUT OF BOUNDS!\n",
      "Drone starting at: (31.88, 41.39) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 91.6 deg. Moving Platform: True\n",
      "Step 60: OUT OF BOUNDS!\n",
      "Drone starting at: (29.20, 46.80) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 70.5 deg. Moving Platform: True\n",
      "Step 80: LOST CONTROL!\n",
      "Drone starting at: (26.01, 47.73) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 304.0 deg. Moving Platform: True\n",
      "Step 76: OUT OF BOUNDS!\n",
      "Drone starting at: (32.21, 41.37) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 23.2 deg. Moving Platform: True\n",
      "Step 43: OUT OF BOUNDS!\n",
      "Drone starting at: (26.19, 42.23) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 92.6 deg. Moving Platform: True\n",
      "Step 131: OUT OF BOUNDS!\n",
      "Step 58: OUT OF BOUNDS!\n",
      "Drone starting at: (22.97, 35.42) mDrone starting at: (32.86, 45.41) m\n",
      "\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 106.4 deg. Moving Platform: True\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 202.9 deg. Moving Platform: True\n",
      "Step 74: LOST CONTROL!\n",
      "Drone starting at: (20.65, 41.07) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 83.6 deg. Moving Platform: True\n",
      "Step 76: OUT OF BOUNDS!\n",
      "Drone starting at: (24.86, 37.14) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 129.9 deg. Moving Platform: True\n",
      "Step 60: OUT OF BOUNDS!\n",
      "Drone starting at: (26.23, 33.88) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 23.5 deg. Moving Platform: True\n",
      "Step 106: LOST CONTROL!\n",
      "Drone starting at: (26.59, 43.75) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 38.8 deg. Moving Platform: True\n",
      "Step 83: OUT OF BOUNDS!\n",
      "Drone starting at: (19.25, 46.80) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 134.5 deg. Moving Platform: True\n",
      "Step 98: OUT OF BOUNDS!\n",
      "Drone starting at: (19.45, 32.73) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 136.1 deg. Moving Platform: True\n",
      "Step 53: OUT OF BOUNDS!\n",
      "Drone starting at: (18.37, 38.34) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 261.2 deg. Moving Platform: True\n",
      "Step 118: OUT OF BOUNDS!\n",
      "Drone starting at: (20.50, 33.70) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 256.3 deg. Moving Platform: True\n",
      "Step 76: OUT OF BOUNDS!\n",
      "Drone starting at: (26.83, 32.33) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 17.2 deg. Moving Platform: True\n",
      "Step 68: LOST CONTROL!\n",
      "Drone starting at: (23.44, 40.02) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 329.1 deg. Moving Platform: True\n",
      "Step 97: LOST CONTROL!\n",
      "Drone starting at: (19.62, 38.92) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 313.2 deg. Moving Platform: True\n",
      "Step 98: LOST CONTROL!\n",
      "Drone starting at: (28.03, 43.02) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 125.7 deg. Moving Platform: True\n",
      "Step 121: LOST CONTROL!\n",
      "Drone starting at: (31.03, 46.85) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 299.3 deg. Moving Platform: True\n",
      "Step 99: LOST CONTROL!\n",
      "Drone starting at: (23.23, 43.37) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 142.0 deg. Moving Platform: True\n",
      "Step 73: OUT OF BOUNDS!\n",
      "Drone starting at: (29.25, 47.69) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 14.3 deg. Moving Platform: True\n",
      "Step 142: LOST CONTROL!\n",
      "Drone starting at: (20.72, 40.06) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 112.6 deg. Moving Platform: True\n",
      "Step 47: OUT OF BOUNDS!\n",
      "Drone starting at: (21.43, 43.57) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 285.6 deg. Moving Platform: True\n",
      "Step 104: LOST CONTROL!\n",
      "Drone starting at: (18.84, 36.37) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 19.3 deg. Moving Platform: True\n",
      "Step 80: OUT OF BOUNDS!\n",
      "Drone starting at: (32.48, 40.07) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 253.1 deg. Moving Platform: True\n",
      "Step 88: OUT OF BOUNDS!\n",
      "Drone starting at: (31.81, 39.61) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 290.7 deg. Moving Platform: True\n",
      "Step 104: OUT OF BOUNDS!\n",
      "Drone starting at: (19.68, 45.10) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 252.5 deg. Moving Platform: True\n",
      "Step 125: OUT OF BOUNDS!\n",
      "Drone starting at: (24.55, 46.84) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 190.6 deg. Moving Platform: True\n",
      "Step 117: LOST CONTROL!\n",
      "Drone starting at: (32.17, 34.35) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 228.3 deg. Moving Platform: True\n",
      "Step 63: OUT OF BOUNDS!\n",
      "Drone starting at: (17.04, 40.76) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 19.5 deg. Moving Platform: True\n",
      "Step 126: LOST CONTROL!\n",
      "Drone starting at: (21.23, 37.10) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 181.6 deg. Moving Platform: True\n",
      "Step 87: LOST CONTROL!\n",
      "Drone starting at: (29.51, 37.94) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 299.5 deg. Moving Platform: True\n",
      "Step 125: LOST CONTROL!\n",
      "Drone starting at: (25.19, 32.36) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 158.2 deg. Moving Platform: True\n",
      "Step 94: LOST CONTROL!\n",
      "Drone starting at: (27.40, 32.45) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 244.0 deg. Moving Platform: True\n",
      "Step 62: LOST CONTROL!\n",
      "Step 114: LOST CONTROL!\n",
      "Drone starting at: (30.72, 43.68) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 295.6 deg. Moving Platform: True\n",
      "Drone starting at: (22.90, 46.24) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 7.6 deg. Moving Platform: True\n",
      "Step 133: OUT OF BOUNDS!\n",
      "Drone starting at: (22.08, 39.52) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 113.7 deg. Moving Platform: True\n",
      "Step 99: LOST CONTROL!\n",
      "Drone starting at: (24.53, 41.20) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 209.5 deg. Moving Platform: True\n",
      "Step 69: OUT OF BOUNDS!\n",
      "Drone starting at: (18.85, 38.18) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 73.2 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (17.14, 43.94) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 200.3 deg. Moving Platform: True\n",
      "Step 75: OUT OF BOUNDS!\n",
      "Drone starting at: (25.71, 40.68) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 282.7 deg. Moving Platform: True\n",
      "Step 103: OUT OF BOUNDS!\n",
      "Drone starting at: (24.20, 35.10) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 35.6 deg. Moving Platform: True\n",
      "Step 87: LOST CONTROL!\n",
      "Step 94: OUT OF BOUNDS!\n",
      "Drone starting at: (18.15, 38.95) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 94.8 deg. Moving Platform: True\n",
      "Drone starting at: (29.39, 42.97) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 176.9 deg. Moving Platform: True\n",
      "Step 115: OUT OF BOUNDS!\n",
      "Drone starting at: (32.29, 40.14) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 18.5 deg. Moving Platform: True\n",
      "Step 82: LOST CONTROL!\n",
      "Drone starting at: (32.71, 42.36) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 44.9 deg. Moving Platform: True\n",
      "Step 88: OUT OF BOUNDS!\n",
      "Drone starting at: (17.08, 43.62) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 298.2 deg. Moving Platform: True\n",
      "Step 94: LOST CONTROL!\n",
      "Drone starting at: (19.44, 39.84) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 303.1 deg. Moving Platform: True\n",
      "Step 106: OUT OF BOUNDS!\n",
      "Drone starting at: (28.55, 47.55) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 302.0 deg. Moving Platform: True\n",
      "Step 77: OUT OF BOUNDS!\n",
      "Drone starting at: (24.21, 45.10) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 263.2 deg. Moving Platform: True\n",
      "Step 68: OUT OF BOUNDS!\n",
      "Drone starting at: (27.88, 33.16) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 221.4 deg. Moving Platform: True\n",
      "Step 90: LOST CONTROL!\n",
      "Drone starting at: (20.26, 43.20) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 298.3 deg. Moving Platform: True\n",
      "Step 111: OUT OF BOUNDS!\n",
      "Drone starting at: (23.96, 33.18) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 320.6 deg. Moving Platform: True\n",
      "Step 124: OUT OF BOUNDS!\n",
      "Drone starting at: (23.11, 42.54) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 223.8 deg. Moving Platform: True\n",
      "Step 131: LOST CONTROL!\n",
      "Drone starting at: (27.68, 47.95) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 107.3 deg. Moving Platform: True\n",
      "Step 160: LOST CONTROL!\n",
      "Drone starting at: (25.29, 34.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 280.8 deg. Moving Platform: True\n",
      "Step 138: LOST CONTROL!\n",
      "Drone starting at: (31.11, 38.01) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 191.7 deg. Moving Platform: True\n",
      "Step 46: OUT OF BOUNDS!\n",
      "Drone starting at: (28.19, 43.40) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 28.3 deg. Moving Platform: True\n",
      "Step 108: OUT OF BOUNDS!\n",
      "Drone starting at: (29.09, 34.36) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 322.9 deg. Moving Platform: True\n",
      "Step 97: LOST CONTROL!\n",
      "Drone starting at: (29.85, 36.31) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 130.8 deg. Moving Platform: True\n",
      "Step 84: OUT OF BOUNDS!\n",
      "Drone starting at: (23.71, 41.07) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 62.1 deg. Moving Platform: True\n",
      "Step 119: OUT OF BOUNDS!\n",
      "Drone starting at: (19.34, 42.22) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 355.8 deg. Moving Platform: True\n",
      "Step 74: LOST CONTROL!\n",
      "Drone starting at: (23.11, 33.26) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 231.6 deg. Moving Platform: True\n",
      "Step 90: LOST CONTROL!\n",
      "Drone starting at: (18.60, 41.98) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 26.6 deg. Moving Platform: True\n",
      "Step 63: LOST CONTROL!\n",
      "Drone starting at: (23.18, 43.28) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 307.9 deg. Moving Platform: True\n",
      "Step 91: OUT OF BOUNDS!\n",
      "Drone starting at: (18.07, 35.82) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 36.7 deg. Moving Platform: True\n",
      "Step 87: LOST CONTROL!\n",
      "Drone starting at: (24.02, 46.77) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 273.8 deg. Moving Platform: True\n",
      "Step 77: OUT OF BOUNDS!\n",
      "Drone starting at: (28.61, 43.44) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 156.5 deg. Moving Platform: True\n",
      "Step 113: OUT OF BOUNDS!\n",
      "Drone starting at: (26.46, 40.01) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 183.8 deg. Moving Platform: True\n",
      "Step 120: OUT OF BOUNDS!\n",
      "Drone starting at: (30.35, 47.71) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 144.3 deg. Moving Platform: True\n",
      "Step 136: LOST CONTROL!\n",
      "Drone starting at: (21.46, 40.17) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 262.8 deg. Moving Platform: True\n",
      "Step 78: OUT OF BOUNDS!\n",
      "Drone starting at: (27.14, 32.69) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 30.5 deg. Moving Platform: True\n",
      "Step 46: OUT OF BOUNDS!\n",
      "Drone starting at: (19.84, 40.72) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 42.3 deg. Moving Platform: True\n",
      "Step 77: LOST CONTROL!\n",
      "Drone starting at: (32.93, 45.96) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 331.5 deg. Moving Platform: True\n",
      "Step 98: OUT OF BOUNDS!\n",
      "Drone starting at: (27.43, 40.26) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 111.1 deg. Moving Platform: True\n",
      "Step 55: LOST CONTROL!\n",
      "Drone starting at: (22.81, 41.05) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 267.3 deg. Moving Platform: True\n",
      "Step 133: OUT OF BOUNDS!\n",
      "Drone starting at: (24.97, 41.69) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 131.2 deg. Moving Platform: True\n",
      "Step 92: OUT OF BOUNDS!\n",
      "Drone starting at: (31.26, 32.55) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 73.5 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (31.41, 36.15) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 186.1 deg. Moving Platform: True\n",
      "Step 75: OUT OF BOUNDS!\n",
      "Drone starting at: (22.21, 40.26) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 206.3 deg. Moving Platform: True\n",
      "Step 76: LOST CONTROL!\n",
      "Drone starting at: (31.42, 46.29) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 315.2 deg. Moving Platform: True\n",
      "Step 157: OUT OF BOUNDS!\n",
      "Drone starting at: (19.65, 34.57) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 222.0 deg. Moving Platform: True\n",
      "Step 115: LOST CONTROL!\n",
      "Drone starting at: (21.62, 33.36) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 118.1 deg. Moving Platform: True\n",
      "Step 85: LOST CONTROL!\n",
      "Drone starting at: (25.94, 47.10) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 325.5 deg. Moving Platform: True\n",
      "Step 97: OUT OF BOUNDS!\n",
      "Drone starting at: (28.92, 42.49) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 266.6 deg. Moving Platform: True\n",
      "Step 141: OUT OF BOUNDS!\n",
      "Drone starting at: (23.17, 33.95) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 254.4 deg. Moving Platform: True\n",
      "Step 121: LOST CONTROL!\n",
      "Drone starting at: (28.16, 42.58) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 21.2 deg. Moving Platform: True\n",
      "Step 64: LOST CONTROL!\n",
      "Drone starting at: (31.89, 47.06) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 179.6 deg. Moving Platform: True\n",
      "Step 82: OUT OF BOUNDS!\n",
      "Drone starting at: (29.51, 32.43) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 198.3 deg. Moving Platform: True\n",
      "Step 88: LOST CONTROL!\n",
      "Drone starting at: (25.68, 39.15) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 178.9 deg. Moving Platform: True\n",
      "Step 62: OUT OF BOUNDS!\n",
      "Drone starting at: (19.02, 45.32) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 36.4 deg. Moving Platform: True\n",
      "Step 55: LOST CONTROL!\n",
      "Step 80: OUT OF BOUNDS!\n",
      "Drone starting at: (20.46, 46.23) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 295.2 deg. Moving Platform: True\n",
      "Drone starting at: (27.30, 42.57) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 147.4 deg. Moving Platform: True\n",
      "Step 106: LOST CONTROL!\n",
      "Drone starting at: (24.10, 35.23) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 48.1 deg. Moving Platform: True\n",
      "Step 71: OUT OF BOUNDS!\n",
      "Drone starting at: (21.84, 36.27) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 63.4 deg. Moving Platform: True\n",
      "Step 79: OUT OF BOUNDS!\n",
      "Drone starting at: (26.78, 42.47) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 106.5 deg. Moving Platform: True\n",
      "Step 106: LOST CONTROL!\n",
      "Drone starting at: (17.83, 45.91) m\n",
      "Environment Reset. Wind Enabled: True, Wind Dir: 315.3 deg. Moving Platform: True\n",
      "\n",
      "--- Training Finished (or interrupted) ---\n",
      "Saving final model to: models/ppo_drone_final\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Log metrics every 10 updates\u001b[39;49;00m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_ALGORITHM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_Drone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Name for TensorBoard run\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Add the checkpoint callback\u001b[39;49;00m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(MODEL_SAVE_PATH)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# --- Clean up ---\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Close the vectorized environment\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment closed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Script Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:159\u001b[0m, in \u001b[0;36mSubprocVecEnv.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m         remote\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremotes:\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclose\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocesses:\n\u001b[1;32m    161\u001b[0m     process\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/multiprocessing/connection.py:411\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/drone_env/lib/python3.10/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# --- Environment Import ---\n",
    "# IMPORTANT: Adjust this import path based on your project structure\n",
    "from drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Training Configuration ---\n",
    "TOTAL_TIMESTEPS = 1_800_000      # Total steps for training (adjust as needed)\n",
    "MODEL_ALGORITHM = PPO           # Algorithm to use (PPO is a good default)\n",
    "POLICY_TYPE = \"MlpPolicy\"       # Policy type (Multi-Layer Perceptron for vector observations)\n",
    "LOG_DIR = \"logs/drone_ppo/\"     # Directory to save TensorBoard logs\n",
    "MODEL_SAVE_PATH = \"models/ppo_drone_first\" # Path to save the trained model\n",
    "CHECKPOINT_FREQ = 50000         # Save a checkpoint every N steps\n",
    "N_ENVS = 4                      # Number of parallel environments (adjust based on CPU cores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Setup ---\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "    print(\"--- Starting Drone Training ---\")\n",
    "    print(f\"Algorithm: {MODEL_ALGORITHM.__name__}\")\n",
    "    print(f\"Total Timesteps: {TOTAL_TIMESTEPS}\")\n",
    "    print(f\"Environment Config: {ENV_CONFIG}\")\n",
    "    print(f\"Using {N_ENVS} parallel environments.\")\n",
    "\n",
    "    # --- Create Vectorized Environment ---\n",
    "    # Use SubprocVecEnv for true parallelism, DummyVecEnv for debugging\n",
    "    # `env_kwargs` passes the configuration dictionary to each environment instance\n",
    "    env = make_vec_env(\n",
    "        Drone2dEnv,\n",
    "        n_envs=N_ENVS,\n",
    "        seed=0,\n",
    "        vec_env_cls=SubprocVecEnv, # Use SubprocVecEnv for parallel processing\n",
    "        #vec_env_cls=DummyVecEnv, # Use DummyVecEnv for easier debugging\n",
    "        env_kwargs=ENV_CONFIG\n",
    "        )\n",
    "\n",
    "    # --- Setup Model ---\n",
    "    # Define the model with policy type, environment, and logging parameters\n",
    "    # You might need to tune hyperparameters like learning_rate, n_steps, batch_size etc.\n",
    "    model = MODEL_ALGORITHM(\n",
    "        POLICY_TYPE,\n",
    "        env,\n",
    "        verbose=1, # Print training progress\n",
    "        tensorboard_log=LOG_DIR,\n",
    "        # Example of adjusting some hyperparameters (optional):\n",
    "        # learning_rate=3e-4,\n",
    "        # n_steps=2048, # Steps per env before update (adjust based on max_steps and N_ENVS)\n",
    "        # batch_size=64,\n",
    "        # gamma=0.99, # Discount factor\n",
    "        # gae_lambda=0.95,\n",
    "        # ent_coef=0.0, # Entropy coefficient\n",
    "    )\n",
    "\n",
    "    # --- Setup Checkpoint Callback ---\n",
    "    # Saves the model periodically during training\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=max(CHECKPOINT_FREQ // N_ENVS, 1), # Adjust frequency based on N_ENVS\n",
    "        save_path=LOG_DIR,\n",
    "        name_prefix=\"drone_ppo_ckpt\"\n",
    "    )\n",
    "\n",
    "    # --- Train the Agent ---\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    try:\n",
    "        model.learn(\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            log_interval=10, # Log metrics every 10 updates\n",
    "            tb_log_name=f\"{MODEL_ALGORITHM.__name__}_Drone\", # Name for TensorBoard run\n",
    "            callback=checkpoint_callback # Add the checkpoint callback\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error during training: {e} !!!\")\n",
    "        print(\"Attempting to save model before exiting...\")\n",
    "        model.save(f\"{MODEL_SAVE_PATH}_error\")\n",
    "    finally:\n",
    "        # --- Save the Final Model ---\n",
    "        print(\"\\n--- Training Finished (or interrupted) ---\")\n",
    "        print(f\"Saving final model to: {MODEL_SAVE_PATH}\")\n",
    "        model.save(MODEL_SAVE_PATH)\n",
    "\n",
    "        # --- Clean up ---\n",
    "        env.close() # Close the vectorized environment\n",
    "        print(\"Environment closed.\")\n",
    "\n",
    "    print(\"\\n--- Training Script Complete ---\")\n",
    "    print(f\"To monitor training, run: tensorboard --logdir {LOG_DIR}\")\n",
    "\n",
    "# --- END OF FILE train_drone_agent.py ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Explanation and How to Use:**\n",
    "\n",
    "1.  **Save:** Save this code as a Python file (e.g., `train_drone_agent.py`) in your project, likely at the root level or wherever you keep your training scripts.\n",
    "2.  **Adjust Import:** **Crucially**, fix the import path for `Drone2dEnv` at the top of the script to match your project's directory structure. The provided `try...except` block attempts common scenarios.\n",
    "3.  **Configure Training:**\n",
    "    *   `TOTAL_TIMESTEPS`: Set how long you want to train (1.8 million is a reasonable starting point, but might need more or less).\n",
    "    *   `LOG_DIR`: Choose where TensorBoard logs will be saved.\n",
    "    *   `MODEL_SAVE_PATH`: Choose where the final trained model `.zip` file will be saved.\n",
    "    *   `CHECKPOINT_FREQ`: How often to save intermediate models (useful if training crashes).\n",
    "    *   `N_ENVS`: Number of parallel environments. Start with `4` or match the number of CPU cores you have available. Using more environments generally speeds up training but uses more RAM.\n",
    "4.  **Configure Environment:** Modify the `ENV_CONFIG` dictionary to set the desired parameters for the `Drone2dEnv` during training (wind, moving platform, start range, etc.). **Remember to keep `render_sim=False` for efficient training.**\n",
    "5.  **Install Dependencies:** Make sure you have the necessary libraries installed in your environment:\n",
    "    ```bash\n",
    "    pip install stable-baselines3[extra] gym pygame pymunk numpy\n",
    "    pip install 'shimmy>=2.0\n",
    "    # Or if using gymnasium:\n",
    "    # pip install stable-baselines3[extra] gymnasium pygame pymunk numpy\n",
    "    ```\n",
    "    *(You might already have these from setting up the environment)*\n",
    "6.  **Run Training:** Open your terminal, navigate to the directory where you saved the script, and run:\n",
    "    ```bash\n",
    "    python train_drone_agent.py\n",
    "    ```\n",
    "7.  **Monitor with TensorBoard:** While training is running (or after it finishes), open *another* terminal, navigate to the same project directory (or one level above `LOG_DIR`), and run:\n",
    "    ```bash\n",
    "    tensorboard --logdir logs/drone_ppo/\n",
    "    ```\n",
    "    Then open the URL provided by TensorBoard (usually `http://localhost:6006/`) in your web browser to see graphs of the reward, loss functions, episode length, etc. This is essential for understanding if the agent is learning.\n",
    "8.  **Tuning:** If the agent doesn't learn well, you may need to:\n",
    "    *   Adjust the environment parameters (e.g., make the task easier initially by disabling wind or the moving platform).\n",
    "    *   Tune the PPO hyperparameters within the `model = MODEL_ALGORITHM(...)` call (learning rate, rollout buffer size `n_steps`, etc.).\n",
    "    *   Modify the reward function in `drone_2d_env.py` if the current shaping isn't effective.\n",
    "    *   Train for longer (increase `TOTAL_TIMESTEPS`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing/ resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Attempt to load the saved model\n",
    "model_loaded = PPO.load(\"new_agent_2.zip\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key features:**\n",
    "\n",
    "*   Specifies the path to the model you want to load (`LOAD_MODEL_PATH`).\n",
    "*   Specifies a *new* path to save the model after continued training (`NEW_SAVE_PATH`).\n",
    "*   Specifies the number of *additional* timesteps to train for (`ADDITIONAL_TIMESTEPS`).\n",
    "*   Uses the same environment configuration (`ENV_CONFIG`) - **important for compatibility**.\n",
    "*   Sets `reset_num_timesteps=False` in `model.learn()` to ensure the training step count and logs continue correctly.\n",
    "*   Includes error handling if the specified model to load doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO  # Or the algorithm you used (e.g., SAC, TD3)\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# --- Environment Import ---\n",
    "# IMPORTANT: Adjust this import path based on how your project is structured\n",
    "from drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "\n",
    "\n",
    "# --- Training Configuration ---\n",
    "# *** Paths ***\n",
    "LOAD_MODEL_PATH = \"models/ppo_drone_first.zip\" # <-- IMPORTANT: Path to the model saved previously\n",
    "NEW_SAVE_PATH = \"models/ppo_drone_first_continued\" # <-- Path to save the model after *this* training session\n",
    "LOG_DIR = \"logs/drone_ppo/\" # <-- Directory for TensorBoard logs (can be same or new)\n",
    "\n",
    "# *** Training Parameters ***\n",
    "ADDITIONAL_TIMESTEPS = 1_000_000 # <-- How many *more* steps to train\n",
    "MODEL_ALGORITHM = PPO         # <-- Must match the algorithm of the loaded model\n",
    "POLICY_TYPE = \"MlpPolicy\"     # <-- Must match the policy type of the loaded model\n",
    "CHECKPOINT_FREQ = 50000       # Save a checkpoint every N steps (during this session)\n",
    "N_ENVS = 4                    # Number of parallel environments (match previous setup if possible)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Setup ---\n",
    "    os.makedirs(os.path.dirname(NEW_SAVE_PATH), exist_ok=True)\n",
    "    os.makedirs(LOG_DIR, exist_ok=True) # Log dir might already exist\n",
    "\n",
    "    print(\"--- Starting Drone Training Continuation ---\")\n",
    "    print(f\"Attempting to load model from: {LOAD_MODEL_PATH}\")\n",
    "    print(f\"Training for additional {ADDITIONAL_TIMESTEPS} timesteps.\")\n",
    "    print(f\"Environment Config: {ENV_CONFIG}\")\n",
    "\n",
    "    # --- Check if Model Exists ---\n",
    "    if not os.path.exists(LOAD_MODEL_PATH):\n",
    "        print(f\"\\n!!! Error: Model file not found at {LOAD_MODEL_PATH} !!!\")\n",
    "        print(\"Cannot continue training without a model to load.\")\n",
    "        exit()\n",
    "\n",
    "    # --- Create Vectorized Environment ---\n",
    "    # Needs to be created *before* loading the model\n",
    "    try:\n",
    "        env = make_vec_env(\n",
    "            Drone2dEnv,\n",
    "            n_envs=N_ENVS,\n",
    "            seed=0, # Seed can be different, but structure should be same\n",
    "            vec_env_cls=SubprocVecEnv,\n",
    "            env_kwargs=ENV_CONFIG\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error creating environment: {e} !!!\")\n",
    "        exit()\n",
    "\n",
    "    # --- Load Model ---\n",
    "    try:\n",
    "        print(f\"\\nLoading model...\")\n",
    "        # Pass the environment and tensorboard log directory to load\n",
    "        # SB3 automatically detects the algorithm and policy type from the zip file\n",
    "        model = MODEL_ALGORITHM.load(\n",
    "            LOAD_MODEL_PATH,\n",
    "            env=env, # Link the loaded model to the (new) environment instances\n",
    "            tensorboard_log=LOG_DIR # Tell SB3 where to continue logging\n",
    "        )\n",
    "        print(f\"Model loaded successfully. Current Timesteps: {model.num_timesteps}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error loading model from {LOAD_MODEL_PATH}: {e} !!!\")\n",
    "        print(\"Check if the file exists, is a valid SB3 model, and matches the environment structure.\")\n",
    "        env.close()\n",
    "        exit()\n",
    "\n",
    "    # --- Setup Checkpoint Callback ---\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=max(CHECKPOINT_FREQ // N_ENVS, 1),\n",
    "        save_path=LOG_DIR, # Save checkpoints in the log directory\n",
    "        name_prefix=f\"{os.path.basename(NEW_SAVE_PATH)}_ckpt\" # Prefix based on new save name\n",
    "    )\n",
    "\n",
    "    # --- Continue Training ---\n",
    "    print(\"\\n--- Continuing Training ---\")\n",
    "    start_timesteps = model.num_timesteps # Get current steps from loaded model\n",
    "    target_timesteps = start_timesteps + ADDITIONAL_TIMESTEPS\n",
    "    print(f\"Training from {start_timesteps} to {target_timesteps} total steps.\")\n",
    "\n",
    "    try:\n",
    "        model.learn(\n",
    "            total_timesteps=ADDITIONAL_TIMESTEPS, # Train for the *additional* steps\n",
    "            log_interval=10,\n",
    "            tb_log_name=f\"{MODEL_ALGORITHM.__name__}_Drone\", # Use same or new log name\n",
    "            reset_num_timesteps=False, # <-- IMPORTANT: Do NOT reset timestep counter\n",
    "            callback=checkpoint_callback\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error during continued training: {e} !!!\")\n",
    "        print(\"Attempting to save model before exiting...\")\n",
    "        model.save(f\"{NEW_SAVE_PATH}_error\")\n",
    "    finally:\n",
    "        # --- Save the Final Model ---\n",
    "        print(\"\\n--- Training Finished (or interrupted) ---\")\n",
    "        print(f\"Saving final model to: {NEW_SAVE_PATH}.zip\") # SB3 adds .zip automatically\n",
    "        model.save(NEW_SAVE_PATH) # Use the NEW save path\n",
    "\n",
    "        # --- Clean up ---\n",
    "        env.close()\n",
    "        print(\"Environment closed.\")\n",
    "\n",
    "    print(\"\\n--- Training Continuation Script Complete ---\")\n",
    "    print(f\"To monitor training, run: tensorboard --logdir {LOG_DIR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation and How to Use:**\n",
    "\n",
    "1.  **Save:** Save this code as `continue_training.py` (or similar) in your project.\n",
    "2.  **Configure Paths:**\n",
    "    *   Set `LOAD_MODEL_PATH` to the exact path of the `.zip` file generated by your previous training run (e.g., `models/ppo_drone_final.zip`).\n",
    "    *   Set `NEW_SAVE_PATH` to where you want the model saved *after this continuation*. It's generally good practice to give it a distinct name initially (e.g., `models/ppo_drone_final_continued`).\n",
    "    *   Set `LOG_DIR` (usually the same as before, so TensorBoard shows one continuous graph).\n",
    "3.  **Configure Training:**\n",
    "    *   Set `ADDITIONAL_TIMESTEPS` to the number of *extra* steps you want to train for.\n",
    "    *   Ensure `MODEL_ALGORITHM` matches the algorithm used to create the loaded model (the script will load it automatically, but it's good practice).\n",
    "4.  **Configure Environment (`ENV_CONFIG`):** Make sure this dictionary matches the settings used during the *original* training run, or at least is compatible (same observation/action space dimensions).\n",
    "5.  **Run:** Execute the script from your terminal:\n",
    "    ```bash\n",
    "    python continue_training.py\n",
    "    ```\n",
    "6.  **Monitor:** Use TensorBoard as before, pointing it to the `LOG_DIR`. You should see the training metrics continue from where the previous run left off.\n",
    "\n",
    "This script provides a robust way to resume training your drone landing agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Configuration ---\n",
    "# *** Paths ***\n",
    "MODEL_PATH = \"models/ppo_drone_first.zip\" # <-- IMPORTANT: Path to the trained model to evaluate\n",
    "# MODEL_PATH = \"logs/drone_ppo/drone_ppo_ckpt_100000_steps.zip\" # Example for loading a checkpoint\n",
    "\n",
    "# *** Evaluation Parameters ***\n",
    "NUM_EVAL_EPISODES = 20        # How many episodes to run for evaluation\n",
    "RENDER_SIM = True             # Set to True to watch the agent perform\n",
    "RENDER_DELAY_S = 0.03         # Delay between frames when rendering (seconds)\n",
    "\n",
    "# *** Environment Configuration for Evaluation ***\n",
    "# Use settings representative of how you expect the agent to perform,\n",
    "# or the specific conditions you want to test.\n",
    "# Often similar or identical to training config, but render_sim=True here.\n",
    "ENV_CONFIG_EVAL = {\n",
    "    \"render_sim\": RENDER_SIM,         # Use the RENDER_SIM flag here\n",
    "    \"max_steps\": 1000,                # Allow more steps for evaluation if needed\n",
    "    \"render_path\": True,              # Enable path/shade rendering for visualization\n",
    "    \"render_shade\": True,\n",
    "    \"shade_distance_m\": 2.0,\n",
    "    \"moving_platform\": True,\n",
    "    \"platform_speed\": 2.0,\n",
    "    \"initial_pos_random_range_m\": 8.0,\n",
    "    \"max_allowed_tilt_angle_rad\": 1.5, # Approx 86 deg\n",
    "    \"enable_wind\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- START OF FILE evaluate_agent.py ---\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3 import PPO # Or the algorithm you used (e.g., SAC, TD3)\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # Evaluation often uses one env\n",
    "\n",
    "# --- Environment Import ---\n",
    "\n",
    "from drone_2d_custom_gym_env.drone_2d_env import Drone2dEnv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Helper Function ---\n",
    "def evaluate_agent(model, env, num_episodes=10, render=False, delay=0.0):\n",
    "    \"\"\"\n",
    "    Evaluates a Stable Baselines3 agent.\n",
    "\n",
    "    :param model: The agent to evaluate.\n",
    "    :param env: The environment instance.\n",
    "    :param num_episodes: Number of episodes to run.\n",
    "    :param render: Whether to render the environment.\n",
    "    :param delay: Delay between frames if rendering.\n",
    "    :return: Dictionary containing evaluation statistics.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    successes = 0\n",
    "    failures = {\n",
    "        \"crashed\": 0,\n",
    "        \"lost_control\": 0,\n",
    "        \"out_of_bounds\": 0,\n",
    "        \"battery_empty\": 0,\n",
    "        \"timeout\": 0, # Reached max steps without success/failure\n",
    "        \"other_error\": 0,\n",
    "    }\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        current_episode_reward = 0\n",
    "        current_episode_length = 0\n",
    "        last_info = {} # Store final info dict\n",
    "\n",
    "        print(f\"  Starting Eval Episode {episode + 1}/{num_episodes}...\")\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                try:\n",
    "                    env.render()\n",
    "                    if delay > 0:\n",
    "                        time.sleep(delay)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error rendering: {e}\")\n",
    "                    render = False # Stop rendering if it errors\n",
    "\n",
    "            # Use deterministic actions for evaluation\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "            try:\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                current_episode_reward += reward\n",
    "                current_episode_length += 1\n",
    "                last_info = info # Update last info each step\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during env.step(): {e}. Terminating episode.\")\n",
    "                 done = True\n",
    "                 last_info['error'] = f\"Exception during step: {e}\"\n",
    "                 failures[\"other_error\"] += 1\n",
    "\n",
    "        # --- Episode Finished ---\n",
    "        episode_rewards.append(current_episode_reward)\n",
    "        episode_lengths.append(current_episode_length)\n",
    "\n",
    "        # Check outcome based on the *last* info dict before done=True\n",
    "        if last_info.get('landed_safely', False):\n",
    "            successes += 1\n",
    "            print(f\"  Episode {episode + 1}: SUCCESS (Landed Safely)\")\n",
    "        elif last_info.get('crashed', False):\n",
    "            failures[\"crashed\"] += 1\n",
    "            print(f\"  Episode {episode + 1}: FAILED (Crashed)\")\n",
    "        elif last_info.get('lost_control', False):\n",
    "            failures[\"lost_control\"] += 1\n",
    "            print(f\"  Episode {episode + 1}: FAILED (Lost Control)\")\n",
    "        elif last_info.get('out_of_bounds', False):\n",
    "            failures[\"out_of_bounds\"] += 1\n",
    "            print(f\"  Episode {episode + 1}: FAILED (Out of Bounds)\")\n",
    "        elif last_info.get('Battery_empty', False):\n",
    "            failures[\"battery_empty\"] += 1\n",
    "            print(f\"  Episode {episode + 1}: FAILED (Battery Empty)\")\n",
    "        elif current_episode_length >= env.get_attr('max_steps')[0]: # Check max steps from env\n",
    "            failures[\"timeout\"] += 1\n",
    "            print(f\"  Episode {episode + 1}: FAILED (Timeout)\")\n",
    "        elif 'error' not in last_info: # If no other failure condition met\n",
    "             failures[\"other_error\"] += 1\n",
    "             print(f\"  Episode {episode + 1}: FAILED (Unknown - finished without specific failure flag)\")\n",
    "\n",
    "        print(f\"  Episode {episode + 1}: Length={current_episode_length}, Reward={current_episode_reward:.2f}\")\n",
    "\n",
    "    # --- Calculate Statistics ---\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    mean_length = np.mean(episode_lengths)\n",
    "    success_rate = successes / num_episodes\n",
    "\n",
    "    stats = {\n",
    "        \"mean_reward\": mean_reward,\n",
    "        \"std_reward\": std_reward,\n",
    "        \"mean_length\": mean_length,\n",
    "        \"success_rate\": success_rate,\n",
    "        \"successes\": successes,\n",
    "        \"failures\": failures,\n",
    "        \"total_episodes\": num_episodes\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Drone Agent Evaluation ---\")\n",
    "    print(f\"Loading model from: {MODEL_PATH}\")\n",
    "    print(f\"Evaluating for {NUM_EVAL_EPISODES} episodes.\")\n",
    "    print(f\"Evaluation Environment Config: {ENV_CONFIG_EVAL}\")\n",
    "\n",
    "    # --- Check if Model Exists ---\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        print(f\"\\n!!! Error: Model file not found at {MODEL_PATH} !!!\")\n",
    "        exit()\n",
    "\n",
    "    # --- Create Evaluation Environment ---\n",
    "    # Usually only need one environment for evaluation\n",
    "    try:\n",
    "        # Use DummyVecEnv for single environment evaluation\n",
    "        eval_env = DummyVecEnv([lambda: Drone2dEnv(**ENV_CONFIG_EVAL)])\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error creating evaluation environment: {e} !!!\")\n",
    "        exit()\n",
    "\n",
    "    # --- Load Model ---\n",
    "    try:\n",
    "        # Automatically detect algorithm from file\n",
    "        # Note: Loading might require the custom env to be registered if using custom policies,\n",
    "        # but usually works fine with standard policies like MlpPolicy if env structure matches.\n",
    "        loaded_model = PPO.load(MODEL_PATH, env=eval_env)\n",
    "        print(\"\\nModel loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! Error loading model: {e} !!!\")\n",
    "        print(\"Check path, model validity, and environment compatibility.\")\n",
    "        eval_env.close()\n",
    "        exit()\n",
    "\n",
    "    # --- Run Evaluation ---\n",
    "    print(\"\\n--- Running Evaluation Loop ---\")\n",
    "    eval_stats = evaluate_agent(\n",
    "        model=loaded_model,\n",
    "        env=eval_env,\n",
    "        num_episodes=NUM_EVAL_EPISODES,\n",
    "        render=RENDER_SIM,\n",
    "        delay=RENDER_DELAY_S\n",
    "    )\n",
    "\n",
    "    # --- Print Summary ---\n",
    "    print(\"\\n\\n--- Evaluation Summary ---\")\n",
    "    print(f\"Evaluated for {eval_stats['total_episodes']} episodes.\")\n",
    "    print(f\"Mean Reward: {eval_stats['mean_reward']:.2f} +/- {eval_stats['std_reward']:.2f}\")\n",
    "    print(f\"Mean Episode Length: {eval_stats['mean_length']:.1f}\")\n",
    "    print(f\"Success Rate (Landed Safely): {eval_stats['success_rate']:.2%}\")\n",
    "    print(f\"Successful Landings: {eval_stats['successes']}\")\n",
    "    print(\"Failure Breakdown:\")\n",
    "    for reason, count in eval_stats['failures'].items():\n",
    "        if count > 0: # Only print reasons that occurred\n",
    "             print(f\"  - {reason.replace('_', ' ').title()}: {count}\")\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    eval_env.close()\n",
    "    print(\"Evaluation environment closed.\")\n",
    "    print(\"--- Evaluation Script Complete ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
